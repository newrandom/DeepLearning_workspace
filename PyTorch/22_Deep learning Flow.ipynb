{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1bd8e3",
   "metadata": {},
   "source": [
    "## 전체 딥러닝 플로우 구현 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b8e0bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d8b5f",
   "metadata": {},
   "source": [
    "#### 데이터 Load 와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "466acf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8efce935",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7f9a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (img - mean) / std\n",
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e61dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('dataset/', download=True, train=True, \\\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = (0.5,), std = (0.5,))\n",
    "                                # 해당 분포를 따르는 스케일로 변경하겠다.\n",
    "            ]\n",
    "        )),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e54f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('dataset/', train=False, \\\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = (0.5,), std = (0.5,))\n",
    "                                # 해당 분포를 따르는 스케일로 변경하겠다.\n",
    "            ]\n",
    "        )),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566604e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ebc9a42",
   "metadata": {},
   "source": [
    "#### 데이터 확인\n",
    "\n",
    "PyTorch에서는 TF와 이미지를 표현하는데 있어서 차이점이 있음. \n",
    "\n",
    "- TF - (batch, height, width, channel)\n",
    "- PyTorch - (batch, channel, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c2dce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b36bcac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "296f8f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e69c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_image = torch.squeeze(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "398c5a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55c3a80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1.], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_image.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "63220ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch_image.numpy()\n",
    "label = labels[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd622e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(label)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89123c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f6724",
   "metadata": {},
   "source": [
    "### 모델 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "46bb72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "344d8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv = nn.Conv2d(params)\n",
    "# conv(x)\n",
    "\n",
    "# F.relu(x)\n",
    "# F.max_pool2d(x, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "edef14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = tf.keras.layers.Conv2D()    # functional\n",
    "# fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d004193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 만들기   \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "                                # pytorch에서는 input, output을 매번 정해줘야 한다.\n",
    "                                # tf.keras.layers.Conv2D(filters=)\n",
    "                                # tf.keras.layers.Dense(filters=100)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))   # activation\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b874b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1af2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0dabad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6002da",
   "metadata": {},
   "source": [
    "### 학습 로직\n",
    "\n",
    "PyTorch에서는 model을 Training 모드로 변경 후 Training 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd36b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch\n",
    "#  - batch\n",
    "#   - model\n",
    "#   - loss\n",
    "#   - grad\n",
    "#   - model update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11538b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0dc020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), 0.03)\n",
    "                # model의 weight를 등록하고\n",
    "                                    # learning rate를 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82decf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "404af925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d107ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x164bd24a0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f047e8",
   "metadata": {},
   "source": [
    "#### Training mode로 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e94700fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981f567",
   "metadata": {},
   "source": [
    "#### Evaluation mode로 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e50abbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8e93862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss : 2.297837257385254\n",
      "batch 1 loss : 2.2959389686584473\n",
      "batch 2 loss : 2.286917209625244\n",
      "batch 3 loss : 2.322261095046997\n",
      "batch 4 loss : 2.294482707977295\n",
      "batch 5 loss : 2.2799930572509766\n",
      "batch 6 loss : 2.28843092918396\n",
      "batch 7 loss : 2.3036673069000244\n",
      "batch 8 loss : 2.291792631149292\n",
      "batch 9 loss : 2.2992866039276123\n",
      "batch 10 loss : 2.284848213195801\n",
      "batch 11 loss : 2.289210319519043\n",
      "batch 12 loss : 2.279237747192383\n",
      "batch 13 loss : 2.2654521465301514\n",
      "batch 14 loss : 2.261046886444092\n",
      "batch 15 loss : 2.245978832244873\n",
      "batch 16 loss : 2.263583183288574\n",
      "batch 17 loss : 2.2681708335876465\n",
      "batch 18 loss : 2.247087240219116\n",
      "batch 19 loss : 2.275162935256958\n",
      "batch 20 loss : 2.233429193496704\n",
      "batch 21 loss : 2.236748456954956\n",
      "batch 22 loss : 2.247743844985962\n",
      "batch 23 loss : 2.2445061206817627\n",
      "batch 24 loss : 2.229224920272827\n",
      "batch 25 loss : 2.205974817276001\n",
      "batch 26 loss : 2.2017130851745605\n",
      "batch 27 loss : 2.1929433345794678\n",
      "batch 28 loss : 2.254809617996216\n",
      "batch 29 loss : 2.2421889305114746\n",
      "batch 30 loss : 2.1874847412109375\n",
      "batch 31 loss : 2.2214255332946777\n",
      "batch 32 loss : 2.1534509658813477\n",
      "batch 33 loss : 2.2019574642181396\n",
      "batch 34 loss : 2.1713268756866455\n",
      "batch 35 loss : 2.2288148403167725\n",
      "batch 36 loss : 2.162243127822876\n",
      "batch 37 loss : 2.18451189994812\n",
      "batch 38 loss : 2.1511452198028564\n",
      "batch 39 loss : 2.1970605850219727\n",
      "batch 40 loss : 2.113894462585449\n",
      "batch 41 loss : 2.137460947036743\n",
      "batch 42 loss : 2.1661317348480225\n",
      "batch 43 loss : 2.164250135421753\n",
      "batch 44 loss : 2.096257448196411\n",
      "batch 45 loss : 2.169170379638672\n",
      "batch 46 loss : 2.093505620956421\n",
      "batch 47 loss : 2.037816047668457\n",
      "batch 48 loss : 2.077604293823242\n",
      "batch 49 loss : 2.0831329822540283\n",
      "batch 50 loss : 2.062236785888672\n",
      "batch 51 loss : 2.04689884185791\n",
      "batch 52 loss : 2.059541702270508\n",
      "batch 53 loss : 1.960639238357544\n",
      "batch 54 loss : 2.026247024536133\n",
      "batch 55 loss : 1.980607271194458\n",
      "batch 56 loss : 1.932341456413269\n",
      "batch 57 loss : 1.9800220727920532\n",
      "batch 58 loss : 1.9822945594787598\n",
      "batch 59 loss : 1.840853214263916\n",
      "batch 60 loss : 1.7993347644805908\n",
      "batch 61 loss : 1.8799211978912354\n",
      "batch 62 loss : 1.738629937171936\n",
      "batch 63 loss : 1.624613881111145\n",
      "batch 64 loss : 1.7243626117706299\n",
      "batch 65 loss : 1.685416340827942\n",
      "batch 66 loss : 1.7131365537643433\n",
      "batch 67 loss : 1.721392273902893\n",
      "batch 68 loss : 1.5927422046661377\n",
      "batch 69 loss : 1.682060956954956\n",
      "batch 70 loss : 1.6037758588790894\n",
      "batch 71 loss : 1.5747777223587036\n",
      "batch 72 loss : 1.418642520904541\n",
      "batch 73 loss : 1.5008381605148315\n",
      "batch 74 loss : 1.6729315519332886\n",
      "batch 75 loss : 1.3812015056610107\n",
      "batch 76 loss : 1.4247180223464966\n",
      "batch 77 loss : 1.4413790702819824\n",
      "batch 78 loss : 1.14132559299469\n",
      "batch 79 loss : 1.212518572807312\n",
      "batch 80 loss : 1.2046350240707397\n",
      "batch 81 loss : 1.365220069885254\n",
      "batch 82 loss : 1.1484028100967407\n",
      "batch 83 loss : 1.14912748336792\n",
      "batch 84 loss : 1.0526642799377441\n",
      "batch 85 loss : 0.9057154655456543\n",
      "batch 86 loss : 1.1494239568710327\n",
      "batch 87 loss : 0.9401999115943909\n",
      "batch 88 loss : 0.9782946109771729\n",
      "batch 89 loss : 1.1941190958023071\n",
      "batch 90 loss : 1.2426981925964355\n",
      "batch 91 loss : 0.9992965459823608\n",
      "batch 92 loss : 0.7628307938575745\n",
      "batch 93 loss : 0.8950994610786438\n",
      "batch 94 loss : 1.0038928985595703\n",
      "batch 95 loss : 1.0728200674057007\n",
      "batch 96 loss : 0.9316948652267456\n",
      "batch 97 loss : 0.8739147186279297\n",
      "batch 98 loss : 0.8953225016593933\n",
      "batch 99 loss : 0.8160673975944519\n",
      "batch 100 loss : 0.5959833264350891\n",
      "batch 101 loss : 0.7871595025062561\n",
      "batch 102 loss : 0.5423082113265991\n",
      "batch 103 loss : 0.44114571809768677\n",
      "batch 104 loss : 0.7285856604576111\n",
      "batch 105 loss : 0.8098326325416565\n",
      "batch 106 loss : 1.5347877740859985\n",
      "batch 107 loss : 1.1408600807189941\n",
      "batch 108 loss : 1.2386456727981567\n",
      "batch 109 loss : 0.7468202710151672\n",
      "batch 110 loss : 0.6753149628639221\n",
      "batch 111 loss : 0.6633909940719604\n",
      "batch 112 loss : 0.4621005654335022\n",
      "batch 113 loss : 0.5737504959106445\n",
      "batch 114 loss : 0.7399590611457825\n",
      "batch 115 loss : 0.6611018180847168\n",
      "batch 116 loss : 0.6962351202964783\n",
      "batch 117 loss : 0.5773764252662659\n",
      "batch 118 loss : 0.6953871846199036\n",
      "batch 119 loss : 0.6163721680641174\n",
      "batch 120 loss : 0.7933640480041504\n",
      "batch 121 loss : 0.47236084938049316\n",
      "batch 122 loss : 0.6696579456329346\n",
      "batch 123 loss : 0.5595988631248474\n",
      "batch 124 loss : 0.4733351469039917\n",
      "batch 125 loss : 0.4298565089702606\n",
      "batch 126 loss : 0.9309353828430176\n",
      "batch 127 loss : 0.6625672578811646\n",
      "batch 128 loss : 0.6906355023384094\n",
      "batch 129 loss : 0.7364099621772766\n",
      "batch 130 loss : 0.4762817621231079\n",
      "batch 131 loss : 0.6074015498161316\n",
      "batch 132 loss : 0.23788248002529144\n",
      "batch 133 loss : 0.32333099842071533\n",
      "batch 134 loss : 0.45272740721702576\n",
      "batch 135 loss : 0.6047197580337524\n",
      "batch 136 loss : 0.3637206256389618\n",
      "batch 137 loss : 0.49476227164268494\n",
      "batch 138 loss : 0.4592742323875427\n",
      "batch 139 loss : 0.44092050194740295\n",
      "batch 140 loss : 0.625815749168396\n",
      "batch 141 loss : 0.45476242899894714\n",
      "batch 142 loss : 0.328592985868454\n",
      "batch 143 loss : 0.29770851135253906\n",
      "batch 144 loss : 0.25781872868537903\n",
      "batch 145 loss : 0.4634639620780945\n",
      "batch 146 loss : 0.6846755743026733\n",
      "batch 147 loss : 0.4116811454296112\n",
      "batch 148 loss : 0.7466323971748352\n",
      "batch 149 loss : 0.8181771636009216\n",
      "batch 150 loss : 0.8718345761299133\n",
      "batch 151 loss : 0.42959919571876526\n",
      "batch 152 loss : 0.57020503282547\n",
      "batch 153 loss : 0.2985709011554718\n",
      "batch 154 loss : 0.4599773585796356\n",
      "batch 155 loss : 0.4580640196800232\n",
      "batch 156 loss : 0.4186621606349945\n",
      "batch 157 loss : 0.38946881890296936\n",
      "batch 158 loss : 0.5370997786521912\n",
      "batch 159 loss : 0.5911303162574768\n",
      "batch 160 loss : 0.2719334661960602\n",
      "batch 161 loss : 0.1308288872241974\n",
      "batch 162 loss : 0.48850661516189575\n",
      "batch 163 loss : 0.4671872556209564\n",
      "batch 164 loss : 0.22343052923679352\n",
      "batch 165 loss : 0.3517967462539673\n",
      "batch 166 loss : 0.2614366114139557\n",
      "batch 167 loss : 0.32335731387138367\n",
      "batch 168 loss : 0.35515904426574707\n",
      "batch 169 loss : 0.2908388078212738\n",
      "batch 170 loss : 0.20903348922729492\n",
      "batch 171 loss : 0.3455812931060791\n",
      "batch 172 loss : 0.669930100440979\n",
      "batch 173 loss : 0.8007439970970154\n",
      "batch 174 loss : 0.4181657135486603\n",
      "batch 175 loss : 0.4932403862476349\n",
      "batch 176 loss : 0.4665555953979492\n",
      "batch 177 loss : 0.5134902000427246\n",
      "batch 178 loss : 0.7086151838302612\n",
      "batch 179 loss : 0.26629987359046936\n",
      "batch 180 loss : 0.30185574293136597\n",
      "batch 181 loss : 0.29458993673324585\n",
      "batch 182 loss : 0.48877274990081787\n",
      "batch 183 loss : 0.3273763358592987\n",
      "batch 184 loss : 0.39131492376327515\n",
      "batch 185 loss : 0.695366382598877\n",
      "batch 186 loss : 0.4354792833328247\n",
      "batch 187 loss : 0.3697856664657593\n",
      "batch 188 loss : 0.4767911434173584\n",
      "batch 189 loss : 0.3878932297229767\n",
      "batch 190 loss : 0.1934124380350113\n",
      "batch 191 loss : 0.4046623110771179\n",
      "batch 192 loss : 0.25613752007484436\n",
      "batch 193 loss : 0.20741289854049683\n",
      "batch 194 loss : 0.2121235579252243\n",
      "batch 195 loss : 0.3934292495250702\n",
      "batch 196 loss : 0.3831647038459778\n",
      "batch 197 loss : 0.4202536940574646\n",
      "batch 198 loss : 0.1647166609764099\n",
      "batch 199 loss : 0.5629667639732361\n",
      "batch 200 loss : 0.3921687602996826\n",
      "batch 201 loss : 0.26385384798049927\n",
      "batch 202 loss : 0.3285925090312958\n",
      "batch 203 loss : 0.23125781118869781\n",
      "batch 204 loss : 0.4144311249256134\n",
      "batch 205 loss : 0.24237067997455597\n",
      "batch 206 loss : 0.4420144855976105\n",
      "batch 207 loss : 0.1698601245880127\n",
      "batch 208 loss : 0.4558795690536499\n",
      "batch 209 loss : 0.14293429255485535\n",
      "batch 210 loss : 0.36314237117767334\n",
      "batch 211 loss : 0.38762688636779785\n",
      "batch 212 loss : 0.4403042793273926\n",
      "batch 213 loss : 0.19075365364551544\n",
      "batch 214 loss : 0.5199287533760071\n",
      "batch 215 loss : 0.28160345554351807\n",
      "batch 216 loss : 0.44704198837280273\n",
      "batch 217 loss : 0.3795761466026306\n",
      "batch 218 loss : 0.2622591555118561\n",
      "batch 219 loss : 0.4865495562553406\n",
      "batch 220 loss : 0.4462404251098633\n",
      "batch 221 loss : 0.4408922791481018\n",
      "batch 222 loss : 0.3072913885116577\n",
      "batch 223 loss : 0.2575680613517761\n",
      "batch 224 loss : 0.2760678827762604\n",
      "batch 225 loss : 0.1863429844379425\n",
      "batch 226 loss : 0.27837124466896057\n",
      "batch 227 loss : 0.1538938283920288\n",
      "batch 228 loss : 0.3174418807029724\n",
      "batch 229 loss : 0.18351490795612335\n",
      "batch 230 loss : 0.3602229058742523\n",
      "batch 231 loss : 0.24921421706676483\n",
      "batch 232 loss : 0.2238389402627945\n",
      "batch 233 loss : 0.17652659118175507\n",
      "batch 234 loss : 0.42671313881874084\n",
      "batch 235 loss : 0.08453793823719025\n",
      "batch 236 loss : 0.4715140163898468\n",
      "batch 237 loss : 0.2532199025154114\n",
      "batch 238 loss : 0.37608474493026733\n",
      "batch 239 loss : 0.3704715073108673\n",
      "batch 240 loss : 0.41637128591537476\n",
      "batch 241 loss : 0.6522809267044067\n",
      "batch 242 loss : 0.4002399742603302\n",
      "batch 243 loss : 0.1818859875202179\n",
      "batch 244 loss : 0.38051092624664307\n",
      "batch 245 loss : 0.2818177044391632\n",
      "batch 246 loss : 0.4143489599227905\n",
      "batch 247 loss : 0.4780230224132538\n",
      "batch 248 loss : 0.173924058675766\n",
      "batch 249 loss : 0.5030774474143982\n",
      "batch 250 loss : 0.4164242744445801\n",
      "batch 251 loss : 0.3085590600967407\n",
      "batch 252 loss : 0.2986760437488556\n",
      "batch 253 loss : 0.25043919682502747\n",
      "batch 254 loss : 0.38705140352249146\n",
      "batch 255 loss : 0.49958524107933044\n",
      "batch 256 loss : 0.2006063461303711\n",
      "batch 257 loss : 0.12173018604516983\n",
      "batch 258 loss : 0.1614992916584015\n",
      "batch 259 loss : 0.24896061420440674\n",
      "batch 260 loss : 0.35881495475769043\n",
      "batch 261 loss : 0.25137194991111755\n",
      "batch 262 loss : 0.39437711238861084\n",
      "batch 263 loss : 0.4251243770122528\n",
      "batch 264 loss : 0.22817154228687286\n",
      "batch 265 loss : 0.15494148433208466\n",
      "batch 266 loss : 0.6643298268318176\n",
      "batch 267 loss : 0.308773010969162\n",
      "batch 268 loss : 0.5491093397140503\n",
      "batch 269 loss : 0.2576768696308136\n",
      "batch 270 loss : 0.20932236313819885\n",
      "batch 271 loss : 0.24569809436798096\n",
      "batch 272 loss : 0.15425372123718262\n",
      "batch 273 loss : 0.3321188986301422\n",
      "batch 274 loss : 0.31713438034057617\n",
      "batch 275 loss : 0.3314967751502991\n",
      "batch 276 loss : 0.3176611363887787\n",
      "batch 277 loss : 0.3955835998058319\n",
      "batch 278 loss : 0.24555011093616486\n",
      "batch 279 loss : 0.2669249475002289\n",
      "batch 280 loss : 0.2755002975463867\n",
      "batch 281 loss : 0.16550564765930176\n",
      "batch 282 loss : 0.26442864537239075\n",
      "batch 283 loss : 0.4784727394580841\n",
      "batch 284 loss : 0.16420961916446686\n",
      "batch 285 loss : 0.48134660720825195\n",
      "batch 286 loss : 0.4149909019470215\n",
      "batch 287 loss : 0.42928940057754517\n",
      "batch 288 loss : 0.12767116725444794\n",
      "batch 289 loss : 0.12080138921737671\n",
      "batch 290 loss : 0.12720037996768951\n",
      "batch 291 loss : 0.3902149200439453\n",
      "batch 292 loss : 0.2894098162651062\n",
      "batch 293 loss : 0.348563551902771\n",
      "batch 294 loss : 0.27274090051651\n",
      "batch 295 loss : 0.27740445733070374\n",
      "batch 296 loss : 0.28264209628105164\n",
      "batch 297 loss : 0.5153071880340576\n",
      "batch 298 loss : 0.3242560625076294\n",
      "batch 299 loss : 0.4506913721561432\n",
      "batch 300 loss : 0.20188628137111664\n",
      "batch 301 loss : 0.4120716452598572\n",
      "batch 302 loss : 0.2993834912776947\n",
      "batch 303 loss : 0.10030558705329895\n",
      "batch 304 loss : 0.11686695367097855\n",
      "batch 305 loss : 0.14199712872505188\n",
      "batch 306 loss : 0.05084776133298874\n",
      "batch 307 loss : 0.11845062673091888\n",
      "batch 308 loss : 0.1439245194196701\n",
      "batch 309 loss : 0.22535015642642975\n",
      "batch 310 loss : 0.14974722266197205\n",
      "batch 311 loss : 0.3878669738769531\n",
      "batch 312 loss : 0.18625368177890778\n",
      "batch 313 loss : 0.20108819007873535\n",
      "batch 314 loss : 0.35560762882232666\n",
      "batch 315 loss : 0.48821884393692017\n",
      "batch 316 loss : 0.1162295937538147\n",
      "batch 317 loss : 0.16918174922466278\n",
      "batch 318 loss : 0.39076855778694153\n",
      "batch 319 loss : 0.15185725688934326\n",
      "batch 320 loss : 0.15860198438167572\n",
      "batch 321 loss : 0.08668041229248047\n",
      "batch 322 loss : 0.4213590919971466\n",
      "batch 323 loss : 0.12616737186908722\n",
      "batch 324 loss : 0.21405702829360962\n",
      "batch 325 loss : 0.22604143619537354\n",
      "batch 326 loss : 0.5458046197891235\n",
      "batch 327 loss : 0.1603832095861435\n",
      "batch 328 loss : 0.17196086049079895\n",
      "batch 329 loss : 0.35421693325042725\n",
      "batch 330 loss : 0.16808223724365234\n",
      "batch 331 loss : 0.15981952846050262\n",
      "batch 332 loss : 0.2376146763563156\n",
      "batch 333 loss : 0.1036139726638794\n",
      "batch 334 loss : 0.16678974032402039\n",
      "batch 335 loss : 0.45165038108825684\n",
      "batch 336 loss : 0.1968775987625122\n",
      "batch 337 loss : 0.04287860170006752\n",
      "batch 338 loss : 0.3310377299785614\n",
      "batch 339 loss : 0.2525464594364166\n",
      "batch 340 loss : 0.13761791586875916\n",
      "batch 341 loss : 0.09439270198345184\n",
      "batch 342 loss : 0.2730141878128052\n",
      "batch 343 loss : 0.23977306485176086\n",
      "batch 344 loss : 0.7000766396522522\n",
      "batch 345 loss : 0.2497367262840271\n",
      "batch 346 loss : 0.15337742865085602\n",
      "batch 347 loss : 0.22349107265472412\n",
      "batch 348 loss : 0.21054896712303162\n",
      "batch 349 loss : 0.4736192524433136\n",
      "batch 350 loss : 0.2514130175113678\n",
      "batch 351 loss : 0.14624175429344177\n",
      "batch 352 loss : 0.05551629886031151\n",
      "batch 353 loss : 0.1918579339981079\n",
      "batch 354 loss : 0.13733381032943726\n",
      "batch 355 loss : 0.1373615562915802\n",
      "batch 356 loss : 0.21054328978061676\n",
      "batch 357 loss : 0.08606870472431183\n",
      "batch 358 loss : 0.24399568140506744\n",
      "batch 359 loss : 0.06691782176494598\n",
      "batch 360 loss : 0.3616020083427429\n",
      "batch 361 loss : 0.2076951414346695\n",
      "batch 362 loss : 0.12214198708534241\n",
      "batch 363 loss : 0.287833034992218\n",
      "batch 364 loss : 0.2636603116989136\n",
      "batch 365 loss : 0.38835078477859497\n",
      "batch 366 loss : 0.20853641629219055\n",
      "batch 367 loss : 0.16939236223697662\n",
      "batch 368 loss : 0.12025399506092072\n",
      "batch 369 loss : 0.15841324627399445\n",
      "batch 370 loss : 0.16065286099910736\n",
      "batch 371 loss : 0.22107115387916565\n",
      "batch 372 loss : 0.15358944237232208\n",
      "batch 373 loss : 0.21826165914535522\n",
      "batch 374 loss : 0.1246321052312851\n",
      "batch 375 loss : 0.20580726861953735\n",
      "batch 376 loss : 0.1597549468278885\n",
      "batch 377 loss : 0.0991399735212326\n",
      "batch 378 loss : 0.2253122478723526\n",
      "batch 379 loss : 0.2929432690143585\n",
      "batch 380 loss : 0.08507286012172699\n",
      "batch 381 loss : 0.19364406168460846\n",
      "batch 382 loss : 0.18185311555862427\n",
      "batch 383 loss : 0.06413450092077255\n",
      "batch 384 loss : 0.34442347288131714\n",
      "batch 385 loss : 0.18335002660751343\n",
      "batch 386 loss : 0.3494018614292145\n",
      "batch 387 loss : 0.2436080276966095\n",
      "batch 388 loss : 0.25830915570259094\n",
      "batch 389 loss : 0.14332371950149536\n",
      "batch 390 loss : 0.1701912134885788\n",
      "batch 391 loss : 0.1642153412103653\n",
      "batch 392 loss : 0.30245858430862427\n",
      "batch 393 loss : 0.2659846246242523\n",
      "batch 394 loss : 0.29825031757354736\n",
      "batch 395 loss : 0.28103911876678467\n",
      "batch 396 loss : 0.050305284559726715\n",
      "batch 397 loss : 0.1451808363199234\n",
      "batch 398 loss : 0.1607408970594406\n",
      "batch 399 loss : 0.1888733059167862\n",
      "batch 400 loss : 0.2875049114227295\n",
      "batch 401 loss : 0.10682959109544754\n",
      "batch 402 loss : 0.0932568833231926\n",
      "batch 403 loss : 0.22216147184371948\n",
      "batch 404 loss : 0.14600791037082672\n",
      "batch 405 loss : 0.04126515984535217\n",
      "batch 406 loss : 0.334368497133255\n",
      "batch 407 loss : 0.16532742977142334\n",
      "batch 408 loss : 0.4928419291973114\n",
      "batch 409 loss : 0.27312102913856506\n",
      "batch 410 loss : 0.3222332000732422\n",
      "batch 411 loss : 0.22022861242294312\n",
      "batch 412 loss : 0.08890542387962341\n",
      "batch 413 loss : 0.11900036782026291\n",
      "batch 414 loss : 0.3212568163871765\n",
      "batch 415 loss : 0.2099842131137848\n",
      "batch 416 loss : 0.22581981122493744\n",
      "batch 417 loss : 0.16172227263450623\n",
      "batch 418 loss : 0.6539861559867859\n",
      "batch 419 loss : 0.3570658266544342\n",
      "batch 420 loss : 0.18010099232196808\n",
      "batch 421 loss : 0.17680621147155762\n",
      "batch 422 loss : 0.19317643344402313\n",
      "batch 423 loss : 0.1389969289302826\n",
      "batch 424 loss : 0.07835594564676285\n",
      "batch 425 loss : 0.35784047842025757\n",
      "batch 426 loss : 0.13293443620204926\n",
      "batch 427 loss : 0.15002620220184326\n",
      "batch 428 loss : 0.039224736392498016\n",
      "batch 429 loss : 0.14131206274032593\n",
      "batch 430 loss : 0.12666626274585724\n",
      "batch 431 loss : 0.14191488921642303\n",
      "batch 432 loss : 0.15272584557533264\n",
      "batch 433 loss : 0.3082817494869232\n",
      "batch 434 loss : 0.14889509975910187\n",
      "batch 435 loss : 0.2491389960050583\n",
      "batch 436 loss : 0.16778846085071564\n",
      "batch 437 loss : 0.20059528946876526\n",
      "batch 438 loss : 0.3434772789478302\n",
      "batch 439 loss : 0.28973227739334106\n",
      "batch 440 loss : 0.12092816829681396\n",
      "batch 441 loss : 0.030620617792010307\n",
      "batch 442 loss : 0.11711504310369492\n",
      "batch 443 loss : 0.1511390060186386\n",
      "batch 444 loss : 0.2957056164741516\n",
      "batch 445 loss : 0.3454686999320984\n",
      "batch 446 loss : 0.3060738146305084\n",
      "batch 447 loss : 0.34765636920928955\n",
      "batch 448 loss : 0.17349283397197723\n",
      "batch 449 loss : 0.0976468175649643\n",
      "batch 450 loss : 0.0903368815779686\n",
      "batch 451 loss : 0.1412811428308487\n",
      "batch 452 loss : 0.30791139602661133\n",
      "batch 453 loss : 0.16547076404094696\n",
      "batch 454 loss : 0.19496677815914154\n",
      "batch 455 loss : 0.4552440643310547\n",
      "batch 456 loss : 0.17772537469863892\n",
      "batch 457 loss : 0.3295701742172241\n",
      "batch 458 loss : 0.3397015929222107\n",
      "batch 459 loss : 0.09195806831121445\n",
      "batch 460 loss : 0.20614153146743774\n",
      "batch 461 loss : 0.23985004425048828\n",
      "batch 462 loss : 0.15896224975585938\n",
      "batch 463 loss : 0.08733296394348145\n",
      "batch 464 loss : 0.3189326822757721\n",
      "batch 465 loss : 0.35848718881607056\n",
      "batch 466 loss : 0.13972420990467072\n",
      "batch 467 loss : 0.3904334008693695\n",
      "batch 468 loss : 0.25938665866851807\n",
      "batch 469 loss : 0.10996432602405548\n",
      "batch 470 loss : 0.2078363597393036\n",
      "batch 471 loss : 0.2554672956466675\n",
      "batch 472 loss : 0.37853023409843445\n",
      "batch 473 loss : 0.15764538943767548\n",
      "batch 474 loss : 0.4230351150035858\n",
      "batch 475 loss : 0.18422996997833252\n",
      "batch 476 loss : 0.11977723985910416\n",
      "batch 477 loss : 0.23476602137088776\n",
      "batch 478 loss : 0.0429256446659565\n",
      "batch 479 loss : 0.15316450595855713\n",
      "batch 480 loss : 0.09321419149637222\n",
      "batch 481 loss : 0.3795531988143921\n",
      "batch 482 loss : 0.15189167857170105\n",
      "batch 483 loss : 0.2590160667896271\n",
      "batch 484 loss : 0.20589514076709747\n",
      "batch 485 loss : 0.33260297775268555\n",
      "batch 486 loss : 0.03338631987571716\n",
      "batch 487 loss : 0.2353384643793106\n",
      "batch 488 loss : 0.2694505453109741\n",
      "batch 489 loss : 0.12006989866495132\n",
      "batch 490 loss : 0.15516188740730286\n",
      "batch 491 loss : 0.05320994555950165\n",
      "batch 492 loss : 0.10892397165298462\n",
      "batch 493 loss : 0.2380964457988739\n",
      "batch 494 loss : 0.0460955947637558\n",
      "batch 495 loss : 0.0957699790596962\n",
      "batch 496 loss : 0.04344018176198006\n",
      "batch 497 loss : 0.13352352380752563\n",
      "batch 498 loss : 0.1318267583847046\n",
      "batch 499 loss : 0.11754898726940155\n",
      "batch 500 loss : 0.12040567398071289\n",
      "batch 501 loss : 0.10538960993289948\n",
      "batch 502 loss : 0.3901018500328064\n",
      "batch 503 loss : 0.1628302037715912\n",
      "batch 504 loss : 0.17176780104637146\n",
      "batch 505 loss : 0.17844563722610474\n",
      "batch 506 loss : 0.08122125267982483\n",
      "batch 507 loss : 0.19222350418567657\n",
      "batch 508 loss : 0.2246963530778885\n",
      "batch 509 loss : 0.21552111208438873\n",
      "batch 510 loss : 0.17194946110248566\n",
      "batch 511 loss : 0.1717437207698822\n",
      "batch 512 loss : 0.1464620977640152\n",
      "batch 513 loss : 0.23416832089424133\n",
      "batch 514 loss : 0.0647583082318306\n",
      "batch 515 loss : 0.1518266797065735\n",
      "batch 516 loss : 0.09292225539684296\n",
      "batch 517 loss : 0.12804247438907623\n",
      "batch 518 loss : 0.11662754416465759\n",
      "batch 519 loss : 0.28893551230430603\n",
      "batch 520 loss : 0.1345255970954895\n",
      "batch 521 loss : 0.1787222921848297\n",
      "batch 522 loss : 0.11372891068458557\n",
      "batch 523 loss : 0.10356772691011429\n",
      "batch 524 loss : 0.08985091745853424\n",
      "batch 525 loss : 0.36265942454338074\n",
      "batch 526 loss : 0.11550974100828171\n",
      "batch 527 loss : 0.4523871839046478\n",
      "batch 528 loss : 0.400532603263855\n",
      "batch 529 loss : 0.1526641696691513\n",
      "batch 530 loss : 0.17687372863292694\n",
      "batch 531 loss : 0.044913843274116516\n",
      "batch 532 loss : 0.147071972489357\n",
      "batch 533 loss : 0.09312461316585541\n",
      "batch 534 loss : 0.33155301213264465\n",
      "batch 535 loss : 0.1814699023962021\n",
      "batch 536 loss : 0.18660026788711548\n",
      "batch 537 loss : 0.18606065213680267\n",
      "batch 538 loss : 0.10457966476678848\n",
      "batch 539 loss : 0.16340798139572144\n",
      "batch 540 loss : 0.13873082399368286\n",
      "batch 541 loss : 0.2880513668060303\n",
      "batch 542 loss : 0.24433493614196777\n",
      "batch 543 loss : 0.3402289152145386\n",
      "batch 544 loss : 0.08592092990875244\n",
      "batch 545 loss : 0.3194590210914612\n",
      "batch 546 loss : 0.10340620577335358\n",
      "batch 547 loss : 0.036715518683195114\n",
      "batch 548 loss : 0.05787624791264534\n",
      "batch 549 loss : 0.1387706696987152\n",
      "batch 550 loss : 0.2647428512573242\n",
      "batch 551 loss : 0.27843841910362244\n",
      "batch 552 loss : 0.15674880146980286\n",
      "batch 553 loss : 0.2527647316455841\n",
      "batch 554 loss : 0.1952660232782364\n",
      "batch 555 loss : 0.04987050220370293\n",
      "batch 556 loss : 0.22704139351844788\n",
      "batch 557 loss : 0.1411457359790802\n",
      "batch 558 loss : 0.23060378432273865\n",
      "batch 559 loss : 0.09487415850162506\n",
      "batch 560 loss : 0.08103841543197632\n",
      "batch 561 loss : 0.1995159387588501\n",
      "batch 562 loss : 0.18258365988731384\n",
      "batch 563 loss : 0.10536305606365204\n",
      "batch 564 loss : 0.2247174084186554\n",
      "batch 565 loss : 0.04068223387002945\n",
      "batch 566 loss : 0.04254938289523125\n",
      "batch 567 loss : 0.18323269486427307\n",
      "batch 568 loss : 0.21545952558517456\n",
      "batch 569 loss : 0.41193971037864685\n",
      "batch 570 loss : 0.29227903485298157\n",
      "batch 571 loss : 0.055865611881017685\n",
      "batch 572 loss : 0.0517798475921154\n",
      "batch 573 loss : 0.31661900877952576\n",
      "batch 574 loss : 0.2869933545589447\n",
      "batch 575 loss : 0.14725345373153687\n",
      "batch 576 loss : 0.07119634002447128\n",
      "batch 577 loss : 0.17264434695243835\n",
      "batch 578 loss : 0.11529488861560822\n",
      "batch 579 loss : 0.1113814264535904\n",
      "batch 580 loss : 0.10951094329357147\n",
      "batch 581 loss : 0.05004526302218437\n",
      "batch 582 loss : 0.34285369515419006\n",
      "batch 583 loss : 0.10504373162984848\n",
      "batch 584 loss : 0.105647511780262\n",
      "batch 585 loss : 0.13389521837234497\n",
      "batch 586 loss : 0.03627586364746094\n",
      "batch 587 loss : 0.17281827330589294\n",
      "batch 588 loss : 0.32330673933029175\n",
      "batch 589 loss : 0.2520790696144104\n",
      "batch 590 loss : 0.06759089231491089\n",
      "batch 591 loss : 0.1828080415725708\n",
      "batch 592 loss : 0.18937984108924866\n",
      "batch 593 loss : 0.1727651208639145\n",
      "batch 594 loss : 0.30298087000846863\n",
      "batch 595 loss : 0.11939355731010437\n",
      "batch 596 loss : 0.1340235471725464\n",
      "batch 597 loss : 0.0861876830458641\n",
      "batch 598 loss : 0.08469734340906143\n",
      "batch 599 loss : 0.11826561391353607\n",
      "batch 600 loss : 0.4590676724910736\n",
      "batch 601 loss : 0.20258547365665436\n",
      "batch 602 loss : 0.07297451049089432\n",
      "batch 603 loss : 0.044472794979810715\n",
      "batch 604 loss : 0.1150319054722786\n",
      "batch 605 loss : 0.09177564829587936\n",
      "batch 606 loss : 0.16632935404777527\n",
      "batch 607 loss : 0.30572497844696045\n",
      "batch 608 loss : 0.20203115046024323\n",
      "batch 609 loss : 0.440425306558609\n",
      "batch 610 loss : 0.14566759765148163\n",
      "batch 611 loss : 0.05180246755480766\n",
      "batch 612 loss : 0.25453564524650574\n",
      "batch 613 loss : 0.2579605281352997\n",
      "batch 614 loss : 0.12882111966609955\n",
      "batch 615 loss : 0.10675886273384094\n",
      "batch 616 loss : 0.10490188747644424\n",
      "batch 617 loss : 0.4082380533218384\n",
      "batch 618 loss : 0.2026081681251526\n",
      "batch 619 loss : 0.06341508775949478\n",
      "batch 620 loss : 0.181954488158226\n",
      "batch 621 loss : 0.20845896005630493\n",
      "batch 622 loss : 0.1236567497253418\n",
      "batch 623 loss : 0.306162565946579\n",
      "batch 624 loss : 0.18876783549785614\n",
      "batch 625 loss : 0.13571614027023315\n",
      "batch 626 loss : 0.11731798946857452\n",
      "batch 627 loss : 0.15026326477527618\n",
      "batch 628 loss : 0.12041634321212769\n",
      "batch 629 loss : 0.13969957828521729\n",
      "batch 630 loss : 0.15150748193264008\n",
      "batch 631 loss : 0.06614609062671661\n",
      "batch 632 loss : 0.3272956311702728\n",
      "batch 633 loss : 0.4036422073841095\n",
      "batch 634 loss : 0.13186603784561157\n",
      "batch 635 loss : 0.033559419214725494\n",
      "batch 636 loss : 0.03554341942071915\n",
      "batch 637 loss : 0.036602701991796494\n",
      "batch 638 loss : 0.1930389404296875\n",
      "batch 639 loss : 0.3140411972999573\n",
      "batch 640 loss : 0.020644325762987137\n",
      "batch 641 loss : 0.042771629989147186\n",
      "batch 642 loss : 0.10310261696577072\n",
      "batch 643 loss : 0.28569090366363525\n",
      "batch 644 loss : 0.10604153573513031\n",
      "batch 645 loss : 0.1666164994239807\n",
      "batch 646 loss : 0.10478287190198898\n",
      "batch 647 loss : 0.1736932396888733\n",
      "batch 648 loss : 0.03578983247280121\n",
      "batch 649 loss : 0.12014147639274597\n",
      "batch 650 loss : 0.14542438089847565\n",
      "batch 651 loss : 0.16300174593925476\n",
      "batch 652 loss : 0.039122238755226135\n",
      "batch 653 loss : 0.3742055892944336\n",
      "batch 654 loss : 0.08419212698936462\n",
      "batch 655 loss : 0.06149553507566452\n",
      "batch 656 loss : 0.06822125613689423\n",
      "batch 657 loss : 0.04386783018708229\n",
      "batch 658 loss : 0.11581965535879135\n",
      "batch 659 loss : 0.4858776032924652\n",
      "batch 660 loss : 0.10113396495580673\n",
      "batch 661 loss : 0.07832569628953934\n",
      "batch 662 loss : 0.2660186290740967\n",
      "batch 663 loss : 0.15076065063476562\n",
      "batch 664 loss : 0.36036568880081177\n",
      "batch 665 loss : 0.04065778851509094\n",
      "batch 666 loss : 0.241655170917511\n",
      "batch 667 loss : 0.09708794951438904\n",
      "batch 668 loss : 0.1151813492178917\n",
      "batch 669 loss : 0.08375172317028046\n",
      "batch 670 loss : 0.1392088532447815\n",
      "batch 671 loss : 0.16727647185325623\n",
      "batch 672 loss : 0.12952181696891785\n",
      "batch 673 loss : 0.16711539030075073\n",
      "batch 674 loss : 0.14003559947013855\n",
      "batch 675 loss : 0.039141856133937836\n",
      "batch 676 loss : 0.13856197893619537\n",
      "batch 677 loss : 0.2933178246021271\n",
      "batch 678 loss : 0.0351438969373703\n",
      "batch 679 loss : 0.16705244779586792\n",
      "batch 680 loss : 0.14136789739131927\n",
      "batch 681 loss : 0.26464134454727173\n",
      "batch 682 loss : 0.16258573532104492\n",
      "batch 683 loss : 0.09614753723144531\n",
      "batch 684 loss : 0.21023812890052795\n",
      "batch 685 loss : 0.03541254624724388\n",
      "batch 686 loss : 0.13469721376895905\n",
      "batch 687 loss : 0.3459636867046356\n",
      "batch 688 loss : 0.2547493875026703\n",
      "batch 689 loss : 0.3967488706111908\n",
      "batch 690 loss : 0.3208087384700775\n",
      "batch 691 loss : 0.13312773406505585\n",
      "batch 692 loss : 0.3322940170764923\n",
      "batch 693 loss : 0.1606629639863968\n",
      "batch 694 loss : 0.06778788566589355\n",
      "batch 695 loss : 0.1100972518324852\n",
      "batch 696 loss : 0.18816044926643372\n",
      "batch 697 loss : 0.27465641498565674\n",
      "batch 698 loss : 0.054263949394226074\n",
      "batch 699 loss : 0.272173136472702\n",
      "batch 700 loss : 0.4258463680744171\n",
      "batch 701 loss : 0.09406325221061707\n",
      "batch 702 loss : 0.2731087803840637\n",
      "batch 703 loss : 0.14672031998634338\n",
      "batch 704 loss : 0.12625287473201752\n",
      "batch 705 loss : 0.13478931784629822\n",
      "batch 706 loss : 0.0510808490216732\n",
      "batch 707 loss : 0.09811757504940033\n",
      "batch 708 loss : 0.06586450338363647\n",
      "batch 709 loss : 0.05791594088077545\n",
      "batch 710 loss : 0.0956810936331749\n",
      "batch 711 loss : 0.16543233394622803\n",
      "batch 712 loss : 0.08108582347631454\n",
      "batch 713 loss : 0.0829373374581337\n",
      "batch 714 loss : 0.04102730378508568\n",
      "batch 715 loss : 0.05704500898718834\n",
      "batch 716 loss : 0.05635317787528038\n",
      "batch 717 loss : 0.05488602817058563\n",
      "batch 718 loss : 0.3386366069316864\n",
      "batch 719 loss : 0.2947460114955902\n",
      "batch 720 loss : 0.09205284714698792\n",
      "batch 721 loss : 0.201056569814682\n",
      "batch 722 loss : 0.12759652733802795\n",
      "batch 723 loss : 0.21916677057743073\n",
      "batch 724 loss : 0.04887523874640465\n",
      "batch 725 loss : 0.1252535730600357\n",
      "batch 726 loss : 0.05460119992494583\n",
      "batch 727 loss : 0.047962889075279236\n",
      "batch 728 loss : 0.24482876062393188\n",
      "batch 729 loss : 0.08210146427154541\n",
      "batch 730 loss : 0.2070401906967163\n",
      "batch 731 loss : 0.05459609255194664\n",
      "batch 732 loss : 0.6208078265190125\n",
      "batch 733 loss : 0.059626467525959015\n",
      "batch 734 loss : 0.15338803827762604\n",
      "batch 735 loss : 0.05614479258656502\n",
      "batch 736 loss : 0.1638849526643753\n",
      "batch 737 loss : 0.19394908845424652\n",
      "batch 738 loss : 0.038379061967134476\n",
      "batch 739 loss : 0.07752595096826553\n",
      "batch 740 loss : 0.19757992029190063\n",
      "batch 741 loss : 0.05534086376428604\n",
      "batch 742 loss : 0.04111824184656143\n",
      "batch 743 loss : 0.2213684320449829\n",
      "batch 744 loss : 0.11396895349025726\n",
      "batch 745 loss : 0.13884808123111725\n",
      "batch 746 loss : 0.03768498823046684\n",
      "batch 747 loss : 0.06743670254945755\n",
      "batch 748 loss : 0.13087807595729828\n",
      "batch 749 loss : 0.10934659838676453\n",
      "batch 750 loss : 0.16541306674480438\n",
      "batch 751 loss : 0.03792398422956467\n",
      "batch 752 loss : 0.0771212950348854\n",
      "batch 753 loss : 0.08605439215898514\n",
      "batch 754 loss : 0.035611290484666824\n",
      "batch 755 loss : 0.3803931772708893\n",
      "batch 756 loss : 0.19550418853759766\n",
      "batch 757 loss : 0.06356886029243469\n",
      "batch 758 loss : 0.3395620584487915\n",
      "batch 759 loss : 0.04972401633858681\n",
      "batch 760 loss : 0.10150489956140518\n",
      "batch 761 loss : 0.041251543909311295\n",
      "batch 762 loss : 0.36448314785957336\n",
      "batch 763 loss : 0.059719789773225784\n",
      "batch 764 loss : 0.169834166765213\n",
      "batch 765 loss : 0.14606210589408875\n",
      "batch 766 loss : 0.13506551086902618\n",
      "batch 767 loss : 0.06717965751886368\n",
      "batch 768 loss : 0.14717164635658264\n",
      "batch 769 loss : 0.2530323565006256\n",
      "batch 770 loss : 0.1231115534901619\n",
      "batch 771 loss : 0.28113025426864624\n",
      "batch 772 loss : 0.15610308945178986\n",
      "batch 773 loss : 0.11517175287008286\n",
      "batch 774 loss : 0.16379769146442413\n",
      "batch 775 loss : 0.1456848680973053\n",
      "batch 776 loss : 0.03650513291358948\n",
      "batch 777 loss : 0.08230222016572952\n",
      "batch 778 loss : 0.061368051916360855\n",
      "batch 779 loss : 0.31255781650543213\n",
      "batch 780 loss : 0.26906177401542664\n",
      "batch 781 loss : 0.06638503819704056\n",
      "batch 782 loss : 0.10966331511735916\n",
      "batch 783 loss : 0.16776366531848907\n",
      "batch 784 loss : 0.1539463847875595\n",
      "batch 785 loss : 0.1180836483836174\n",
      "batch 786 loss : 0.1300077736377716\n",
      "batch 787 loss : 0.38930439949035645\n",
      "batch 788 loss : 0.09802278876304626\n",
      "batch 789 loss : 0.08905301243066788\n",
      "batch 790 loss : 0.07105838507413864\n",
      "batch 791 loss : 0.25739750266075134\n",
      "batch 792 loss : 0.08372629433870316\n",
      "batch 793 loss : 0.03631702437996864\n",
      "batch 794 loss : 0.028832007199525833\n",
      "batch 795 loss : 0.1352049559354782\n",
      "batch 796 loss : 0.0964246392250061\n",
      "batch 797 loss : 0.1886865198612213\n",
      "batch 798 loss : 0.05447935685515404\n",
      "batch 799 loss : 0.03046787530183792\n",
      "batch 800 loss : 0.06953825056552887\n",
      "batch 801 loss : 0.1563922017812729\n",
      "batch 802 loss : 0.1043303981423378\n",
      "batch 803 loss : 0.07608908414840698\n",
      "batch 804 loss : 0.06155133619904518\n",
      "batch 805 loss : 0.008692598901689053\n",
      "batch 806 loss : 0.2633620500564575\n",
      "batch 807 loss : 0.04559637978672981\n",
      "batch 808 loss : 0.03165033832192421\n",
      "batch 809 loss : 0.02888278104364872\n",
      "batch 810 loss : 0.07451050728559494\n",
      "batch 811 loss : 0.17707252502441406\n",
      "batch 812 loss : 0.016530506312847137\n",
      "batch 813 loss : 0.06462487578392029\n",
      "batch 814 loss : 0.2535918653011322\n",
      "batch 815 loss : 0.0756932944059372\n",
      "batch 816 loss : 0.24244658648967743\n",
      "batch 817 loss : 0.04264218360185623\n",
      "batch 818 loss : 0.0543920136988163\n",
      "batch 819 loss : 0.17028065025806427\n",
      "batch 820 loss : 0.17428287863731384\n",
      "batch 821 loss : 0.2625303864479065\n",
      "batch 822 loss : 0.1049165278673172\n",
      "batch 823 loss : 0.07323240488767624\n",
      "batch 824 loss : 0.06478054076433182\n",
      "batch 825 loss : 0.032299283891916275\n",
      "batch 826 loss : 0.17067919671535492\n",
      "batch 827 loss : 0.04381115734577179\n",
      "batch 828 loss : 0.18419091403484344\n",
      "batch 829 loss : 0.1200210303068161\n",
      "batch 830 loss : 0.010879803448915482\n",
      "batch 831 loss : 0.0719202309846878\n",
      "batch 832 loss : 0.11200655996799469\n",
      "batch 833 loss : 0.047549113631248474\n",
      "batch 834 loss : 0.1305863857269287\n",
      "batch 835 loss : 0.0909361019730568\n",
      "batch 836 loss : 0.13630832731723785\n",
      "batch 837 loss : 0.04366779699921608\n",
      "batch 838 loss : 0.16290375590324402\n",
      "batch 839 loss : 0.23787939548492432\n",
      "batch 840 loss : 0.011182418093085289\n",
      "batch 841 loss : 0.09366002678871155\n",
      "batch 842 loss : 0.14033687114715576\n",
      "batch 843 loss : 0.15054860711097717\n",
      "batch 844 loss : 0.07265534996986389\n",
      "batch 845 loss : 0.1398630142211914\n",
      "batch 846 loss : 0.125711590051651\n",
      "batch 847 loss : 0.02023758366703987\n",
      "batch 848 loss : 0.07444470375776291\n",
      "batch 849 loss : 0.02870938554406166\n",
      "batch 850 loss : 0.027176087722182274\n",
      "batch 851 loss : 0.04922553896903992\n",
      "batch 852 loss : 0.17696912586688995\n",
      "batch 853 loss : 0.03973425552248955\n",
      "batch 854 loss : 0.05645860359072685\n",
      "batch 855 loss : 0.1020388975739479\n",
      "batch 856 loss : 0.14480005204677582\n",
      "batch 857 loss : 0.13145893812179565\n",
      "batch 858 loss : 0.08620727062225342\n",
      "batch 859 loss : 0.07439514994621277\n",
      "batch 860 loss : 0.12190360575914383\n",
      "batch 861 loss : 0.08732105791568756\n",
      "batch 862 loss : 0.20664000511169434\n",
      "batch 863 loss : 0.09692176431417465\n",
      "batch 864 loss : 0.09608521312475204\n",
      "batch 865 loss : 0.44752925634384155\n",
      "batch 866 loss : 0.3587503433227539\n",
      "batch 867 loss : 0.045760564506053925\n",
      "batch 868 loss : 0.06010731682181358\n",
      "batch 869 loss : 0.2934880256652832\n",
      "batch 870 loss : 0.1657201647758484\n",
      "batch 871 loss : 0.04416099935770035\n",
      "batch 872 loss : 0.048211757093667984\n",
      "batch 873 loss : 0.08262158930301666\n",
      "batch 874 loss : 0.017829587683081627\n",
      "batch 875 loss : 0.10517659783363342\n",
      "batch 876 loss : 0.09951747953891754\n",
      "batch 877 loss : 0.026181314140558243\n",
      "batch 878 loss : 0.26137202978134155\n",
      "batch 879 loss : 0.12552034854888916\n",
      "batch 880 loss : 0.08670832216739655\n",
      "batch 881 loss : 0.013644132763147354\n",
      "batch 882 loss : 0.1964184045791626\n",
      "batch 883 loss : 0.037474893033504486\n",
      "batch 884 loss : 0.1898021250963211\n",
      "batch 885 loss : 0.10641348361968994\n",
      "batch 886 loss : 0.09257987886667252\n",
      "batch 887 loss : 0.12089038640260696\n",
      "batch 888 loss : 0.059738535434007645\n",
      "batch 889 loss : 0.02131110429763794\n",
      "batch 890 loss : 0.04178375378251076\n",
      "batch 891 loss : 0.20897026360034943\n",
      "batch 892 loss : 0.11595331132411957\n",
      "batch 893 loss : 0.1346241682767868\n",
      "batch 894 loss : 0.041855230927467346\n",
      "batch 895 loss : 0.15054316818714142\n",
      "batch 896 loss : 0.04617965966463089\n",
      "batch 897 loss : 0.13141626119613647\n",
      "batch 898 loss : 0.0361844003200531\n",
      "batch 899 loss : 0.20307397842407227\n",
      "batch 900 loss : 0.12594406306743622\n",
      "batch 901 loss : 0.255622535943985\n",
      "batch 902 loss : 0.1978713721036911\n",
      "batch 903 loss : 0.21853432059288025\n",
      "batch 904 loss : 0.11323412507772446\n",
      "batch 905 loss : 0.06021590530872345\n",
      "batch 906 loss : 0.11206483095884323\n",
      "batch 907 loss : 0.06771255284547806\n",
      "batch 908 loss : 0.049090560525655746\n",
      "batch 909 loss : 0.32301297783851624\n",
      "batch 910 loss : 0.1581476628780365\n",
      "batch 911 loss : 0.14419890940189362\n",
      "batch 912 loss : 0.13531701266765594\n",
      "batch 913 loss : 0.05089316889643669\n",
      "batch 914 loss : 0.046577777713537216\n",
      "batch 915 loss : 0.1265074759721756\n",
      "batch 916 loss : 0.22646987438201904\n",
      "batch 917 loss : 0.12552031874656677\n",
      "batch 918 loss : 0.08262396603822708\n",
      "batch 919 loss : 0.2614610493183136\n",
      "batch 920 loss : 0.10602446645498276\n",
      "batch 921 loss : 0.02968684211373329\n",
      "batch 922 loss : 0.027394793927669525\n",
      "batch 923 loss : 0.04523254185914993\n",
      "batch 924 loss : 0.2508443593978882\n",
      "batch 925 loss : 0.04420313611626625\n",
      "batch 926 loss : 0.06385502219200134\n",
      "batch 927 loss : 0.1299055814743042\n",
      "batch 928 loss : 0.031325340270996094\n",
      "batch 929 loss : 0.09071468561887741\n",
      "batch 930 loss : 0.11805083602666855\n",
      "batch 931 loss : 0.03827187046408653\n",
      "batch 932 loss : 0.21198000013828278\n",
      "batch 933 loss : 0.07712897658348083\n",
      "batch 934 loss : 0.13262948393821716\n",
      "batch 935 loss : 0.12251678854227066\n",
      "batch 936 loss : 0.13139930367469788\n",
      "batch 937 loss : 0.0697677880525589\n",
      "batch 938 loss : 0.027093594893813133\n",
      "batch 939 loss : 0.055901575833559036\n",
      "batch 940 loss : 0.09329655766487122\n",
      "batch 941 loss : 0.07002249360084534\n",
      "batch 942 loss : 0.2570214569568634\n",
      "batch 943 loss : 0.03691156208515167\n",
      "batch 944 loss : 0.07063450664281845\n",
      "batch 945 loss : 0.02094016596674919\n",
      "batch 946 loss : 0.06613342463970184\n",
      "batch 947 loss : 0.02298499271273613\n",
      "batch 948 loss : 0.18514709174633026\n",
      "batch 949 loss : 0.033003270626068115\n",
      "batch 950 loss : 0.15786579251289368\n",
      "batch 951 loss : 0.09455326199531555\n",
      "batch 952 loss : 0.07964257895946503\n",
      "batch 953 loss : 0.033275991678237915\n",
      "batch 954 loss : 0.07501581311225891\n",
      "batch 955 loss : 0.3342922329902649\n",
      "batch 956 loss : 0.0664423257112503\n",
      "batch 957 loss : 0.08919620513916016\n",
      "batch 958 loss : 0.15562209486961365\n",
      "batch 959 loss : 0.14452224969863892\n",
      "batch 960 loss : 0.042361605912446976\n",
      "batch 961 loss : 0.3523346781730652\n",
      "batch 962 loss : 0.1566212922334671\n",
      "batch 963 loss : 0.07170703262090683\n",
      "batch 964 loss : 0.05870957300066948\n",
      "batch 965 loss : 0.1627846509218216\n",
      "batch 966 loss : 0.058944329619407654\n",
      "batch 967 loss : 0.24977703392505646\n",
      "batch 968 loss : 0.1206434965133667\n",
      "batch 969 loss : 0.01520064938813448\n",
      "batch 970 loss : 0.19026511907577515\n",
      "batch 971 loss : 0.03760524094104767\n",
      "batch 972 loss : 0.06877420842647552\n",
      "batch 973 loss : 0.03504600003361702\n",
      "batch 974 loss : 0.00991358608007431\n",
      "batch 975 loss : 0.06516978144645691\n",
      "batch 976 loss : 0.041871026158332825\n",
      "batch 977 loss : 0.20558181405067444\n",
      "batch 978 loss : 0.24655207991600037\n",
      "batch 979 loss : 0.051222749054431915\n",
      "batch 980 loss : 0.03027261234819889\n",
      "batch 981 loss : 0.21737140417099\n",
      "batch 982 loss : 0.06186837702989578\n",
      "batch 983 loss : 0.1605522185564041\n",
      "batch 984 loss : 0.07155521214008331\n",
      "batch 985 loss : 0.02426840364933014\n",
      "batch 986 loss : 0.18753813207149506\n",
      "batch 987 loss : 0.16146308183670044\n",
      "batch 988 loss : 0.35606080293655396\n",
      "batch 989 loss : 0.20152726769447327\n",
      "batch 990 loss : 0.028016667813062668\n",
      "batch 991 loss : 0.06804229319095612\n",
      "batch 992 loss : 0.10537029802799225\n",
      "batch 993 loss : 0.08307646960020065\n",
      "batch 994 loss : 0.07471486181020737\n",
      "batch 995 loss : 0.052578601986169815\n",
      "batch 996 loss : 0.10153745114803314\n",
      "batch 997 loss : 0.061514370143413544\n",
      "batch 998 loss : 0.2170373499393463\n",
      "batch 999 loss : 0.21873155236244202\n",
      "batch 1000 loss : 0.021379446610808372\n",
      "batch 1001 loss : 0.13567915558815002\n",
      "batch 1002 loss : 0.07246445119380951\n",
      "batch 1003 loss : 0.12204885482788086\n",
      "batch 1004 loss : 0.16852150857448578\n",
      "batch 1005 loss : 0.24786244332790375\n",
      "batch 1006 loss : 0.00990699976682663\n",
      "batch 1007 loss : 0.2734469473361969\n",
      "batch 1008 loss : 0.08620411902666092\n",
      "batch 1009 loss : 0.10352081060409546\n",
      "batch 1010 loss : 0.20805597305297852\n",
      "batch 1011 loss : 0.056955721229314804\n",
      "batch 1012 loss : 0.09432054311037064\n",
      "batch 1013 loss : 0.26681631803512573\n",
      "batch 1014 loss : 0.11244235932826996\n",
      "batch 1015 loss : 0.08363740146160126\n",
      "batch 1016 loss : 0.07040444016456604\n",
      "batch 1017 loss : 0.11224108189344406\n",
      "batch 1018 loss : 0.18216776847839355\n",
      "batch 1019 loss : 0.05460018292069435\n",
      "batch 1020 loss : 0.03872304409742355\n",
      "batch 1021 loss : 0.09479532390832901\n",
      "batch 1022 loss : 0.041797950863838196\n",
      "batch 1023 loss : 0.051147885620594025\n",
      "batch 1024 loss : 0.025873063132166862\n",
      "batch 1025 loss : 0.2855101227760315\n",
      "batch 1026 loss : 0.028582070022821426\n",
      "batch 1027 loss : 0.10673317313194275\n",
      "batch 1028 loss : 0.05902092903852463\n",
      "batch 1029 loss : 0.09062529355287552\n",
      "batch 1030 loss : 0.04668373987078667\n",
      "batch 1031 loss : 0.11589844524860382\n",
      "batch 1032 loss : 0.0424967035651207\n",
      "batch 1033 loss : 0.04486558958888054\n",
      "batch 1034 loss : 0.019323235377669334\n",
      "batch 1035 loss : 0.3314065933227539\n",
      "batch 1036 loss : 0.07482859492301941\n",
      "batch 1037 loss : 0.10251835733652115\n",
      "batch 1038 loss : 0.04562731087207794\n",
      "batch 1039 loss : 0.05476090684533119\n",
      "batch 1040 loss : 0.1526222676038742\n",
      "batch 1041 loss : 0.16434212028980255\n",
      "batch 1042 loss : 0.3152247369289398\n",
      "batch 1043 loss : 0.05841301381587982\n",
      "batch 1044 loss : 0.01494344137609005\n",
      "batch 1045 loss : 0.043808937072753906\n",
      "batch 1046 loss : 0.09757816046476364\n",
      "batch 1047 loss : 0.019819406792521477\n",
      "batch 1048 loss : 0.055255938321352005\n",
      "batch 1049 loss : 0.06049282103776932\n",
      "batch 1050 loss : 0.04856928065419197\n",
      "batch 1051 loss : 0.072357676923275\n",
      "batch 1052 loss : 0.2208797037601471\n",
      "batch 1053 loss : 0.026787061244249344\n",
      "batch 1054 loss : 0.19558842480182648\n",
      "batch 1055 loss : 0.03051152266561985\n",
      "batch 1056 loss : 0.1535579413175583\n",
      "batch 1057 loss : 0.1160459965467453\n",
      "batch 1058 loss : 0.06634068489074707\n",
      "batch 1059 loss : 0.15988285839557648\n",
      "batch 1060 loss : 0.18230634927749634\n",
      "batch 1061 loss : 0.07550886273384094\n",
      "batch 1062 loss : 0.11920268088579178\n",
      "batch 1063 loss : 0.03488321974873543\n",
      "batch 1064 loss : 0.07039880752563477\n",
      "batch 1065 loss : 0.01912105455994606\n",
      "batch 1066 loss : 0.16263455152511597\n",
      "batch 1067 loss : 0.15860100090503693\n",
      "batch 1068 loss : 0.03169127553701401\n",
      "batch 1069 loss : 0.11401568353176117\n",
      "batch 1070 loss : 0.10558243840932846\n",
      "batch 1071 loss : 0.08574554324150085\n",
      "batch 1072 loss : 0.11287075281143188\n",
      "batch 1073 loss : 0.3474254608154297\n",
      "batch 1074 loss : 0.19797277450561523\n",
      "batch 1075 loss : 0.04213108867406845\n",
      "batch 1076 loss : 0.15166348218917847\n",
      "batch 1077 loss : 0.04261196404695511\n",
      "batch 1078 loss : 0.06323789805173874\n",
      "batch 1079 loss : 0.04933006316423416\n",
      "batch 1080 loss : 0.10918467491865158\n",
      "batch 1081 loss : 0.2523530125617981\n",
      "batch 1082 loss : 0.12720398604869843\n",
      "batch 1083 loss : 0.11003643274307251\n",
      "batch 1084 loss : 0.1106434240937233\n",
      "batch 1085 loss : 0.12621699273586273\n",
      "batch 1086 loss : 0.10644134879112244\n",
      "batch 1087 loss : 0.12799617648124695\n",
      "batch 1088 loss : 0.04475267231464386\n",
      "batch 1089 loss : 0.23854675889015198\n",
      "batch 1090 loss : 0.04764588549733162\n",
      "batch 1091 loss : 0.02517072856426239\n",
      "batch 1092 loss : 0.010720182210206985\n",
      "batch 1093 loss : 0.12830939888954163\n",
      "batch 1094 loss : 0.12970253825187683\n",
      "batch 1095 loss : 0.036621518433094025\n",
      "batch 1096 loss : 0.08212364464998245\n",
      "batch 1097 loss : 0.05149146914482117\n",
      "batch 1098 loss : 0.022757310420274734\n",
      "batch 1099 loss : 0.04683951660990715\n",
      "batch 1100 loss : 0.1502847671508789\n",
      "batch 1101 loss : 0.2726297080516815\n",
      "batch 1102 loss : 0.05160553753376007\n",
      "batch 1103 loss : 0.22408606112003326\n",
      "batch 1104 loss : 0.040085259824991226\n",
      "batch 1105 loss : 0.1930323988199234\n",
      "batch 1106 loss : 0.1684221476316452\n",
      "batch 1107 loss : 0.12776291370391846\n",
      "batch 1108 loss : 0.023790182545781136\n",
      "batch 1109 loss : 0.010786767117679119\n",
      "batch 1110 loss : 0.2241639941930771\n",
      "batch 1111 loss : 0.010642739944159985\n",
      "batch 1112 loss : 0.16458436846733093\n",
      "batch 1113 loss : 0.03327930346131325\n",
      "batch 1114 loss : 0.15756386518478394\n",
      "batch 1115 loss : 0.04200134426355362\n",
      "batch 1116 loss : 0.09533189982175827\n",
      "batch 1117 loss : 0.033323705196380615\n",
      "batch 1118 loss : 0.037256788462400436\n",
      "batch 1119 loss : 0.0623956136405468\n",
      "batch 1120 loss : 0.090628482401371\n",
      "batch 1121 loss : 0.07122524827718735\n",
      "batch 1122 loss : 0.11484508216381073\n",
      "batch 1123 loss : 0.03641863167285919\n",
      "batch 1124 loss : 0.037651918828487396\n",
      "batch 1125 loss : 0.06324001401662827\n",
      "batch 1126 loss : 0.14468075335025787\n",
      "batch 1127 loss : 0.060291655361652374\n",
      "batch 1128 loss : 0.09591684490442276\n",
      "batch 1129 loss : 0.2406151294708252\n",
      "batch 1130 loss : 0.07081408053636551\n",
      "batch 1131 loss : 0.2177581638097763\n",
      "batch 1132 loss : 0.5195537805557251\n",
      "batch 1133 loss : 0.2645527720451355\n",
      "batch 1134 loss : 0.042375367134809494\n",
      "batch 1135 loss : 0.20618203282356262\n",
      "batch 1136 loss : 0.2077973484992981\n",
      "batch 1137 loss : 0.23447464406490326\n",
      "batch 1138 loss : 0.15778174996376038\n",
      "batch 1139 loss : 0.09935302287340164\n",
      "batch 1140 loss : 0.03853515908122063\n",
      "batch 1141 loss : 0.03388968110084534\n",
      "batch 1142 loss : 0.03747379779815674\n",
      "batch 1143 loss : 0.04312516748905182\n",
      "batch 1144 loss : 0.06053547561168671\n",
      "batch 1145 loss : 0.035521771758794785\n",
      "batch 1146 loss : 0.10964225977659225\n",
      "batch 1147 loss : 0.02637823298573494\n",
      "batch 1148 loss : 0.05070355907082558\n",
      "batch 1149 loss : 0.04994992911815643\n",
      "batch 1150 loss : 0.13030202686786652\n",
      "batch 1151 loss : 0.1471247673034668\n",
      "batch 1152 loss : 0.39205440878868103\n",
      "batch 1153 loss : 0.11226806044578552\n",
      "batch 1154 loss : 0.017401814460754395\n",
      "batch 1155 loss : 0.20740348100662231\n",
      "batch 1156 loss : 0.04558628052473068\n",
      "batch 1157 loss : 0.01378563791513443\n",
      "batch 1158 loss : 0.12086092680692673\n",
      "batch 1159 loss : 0.024764183908700943\n",
      "batch 1160 loss : 0.13764387369155884\n",
      "batch 1161 loss : 0.15744942426681519\n",
      "batch 1162 loss : 0.14982715249061584\n",
      "batch 1163 loss : 0.0556347481906414\n",
      "batch 1164 loss : 0.035519327968358994\n",
      "batch 1165 loss : 0.06013310328125954\n",
      "batch 1166 loss : 0.031962353736162186\n",
      "batch 1167 loss : 0.013209536671638489\n",
      "batch 1168 loss : 0.03937220200896263\n",
      "batch 1169 loss : 0.06209408864378929\n",
      "batch 1170 loss : 0.05332886427640915\n",
      "batch 1171 loss : 0.013955492526292801\n",
      "batch 1172 loss : 0.01031971350312233\n",
      "batch 1173 loss : 0.060510728508234024\n",
      "batch 1174 loss : 0.08363232761621475\n",
      "batch 1175 loss : 0.07707121223211288\n",
      "batch 1176 loss : 0.12936685979366302\n",
      "batch 1177 loss : 0.0830460861325264\n",
      "batch 1178 loss : 0.19949458539485931\n",
      "batch 1179 loss : 0.08265426754951477\n",
      "batch 1180 loss : 0.0522620864212513\n",
      "batch 1181 loss : 0.029181182384490967\n",
      "batch 1182 loss : 0.16748428344726562\n",
      "batch 1183 loss : 0.07280562072992325\n",
      "batch 1184 loss : 0.12731778621673584\n",
      "batch 1185 loss : 0.1546763926744461\n",
      "batch 1186 loss : 0.2021339237689972\n",
      "batch 1187 loss : 0.1892758011817932\n",
      "batch 1188 loss : 0.2905808985233307\n",
      "batch 1189 loss : 0.06022009253501892\n",
      "batch 1190 loss : 0.09728637337684631\n",
      "batch 1191 loss : 0.025791041553020477\n",
      "batch 1192 loss : 0.3172653615474701\n",
      "batch 1193 loss : 0.09635061770677567\n",
      "batch 1194 loss : 0.08416067063808441\n",
      "batch 1195 loss : 0.03474576026201248\n",
      "batch 1196 loss : 0.04884002357721329\n",
      "batch 1197 loss : 0.2678069472312927\n",
      "batch 1198 loss : 0.19651414453983307\n",
      "batch 1199 loss : 0.0212626401335001\n",
      "batch 1200 loss : 0.18236956000328064\n",
      "batch 1201 loss : 0.10457269847393036\n",
      "batch 1202 loss : 0.06116464361548424\n",
      "batch 1203 loss : 0.017349282279610634\n",
      "batch 1204 loss : 0.1934594362974167\n",
      "batch 1205 loss : 0.09393280744552612\n",
      "batch 1206 loss : 0.2118876874446869\n",
      "batch 1207 loss : 0.013977986760437489\n",
      "batch 1208 loss : 0.12374971061944962\n",
      "batch 1209 loss : 0.3380233943462372\n",
      "batch 1210 loss : 0.2616339921951294\n",
      "batch 1211 loss : 0.1251470446586609\n",
      "batch 1212 loss : 0.08685683459043503\n",
      "batch 1213 loss : 0.24176855385303497\n",
      "batch 1214 loss : 0.05663004890084267\n",
      "batch 1215 loss : 0.0314590185880661\n",
      "batch 1216 loss : 0.024170827120542526\n",
      "batch 1217 loss : 0.04031470790505409\n",
      "batch 1218 loss : 0.06041586026549339\n",
      "batch 1219 loss : 0.03148408234119415\n",
      "batch 1220 loss : 0.005755388177931309\n",
      "batch 1221 loss : 0.03557325527071953\n",
      "batch 1222 loss : 0.060810815542936325\n",
      "batch 1223 loss : 0.05858569219708443\n",
      "batch 1224 loss : 0.23361897468566895\n",
      "batch 1225 loss : 0.1114572286605835\n",
      "batch 1226 loss : 0.03249950706958771\n",
      "batch 1227 loss : 0.1431168168783188\n",
      "batch 1228 loss : 0.07147970795631409\n",
      "batch 1229 loss : 0.19215677678585052\n",
      "batch 1230 loss : 0.09701340645551682\n",
      "batch 1231 loss : 0.11851587146520615\n",
      "batch 1232 loss : 0.2821464240550995\n",
      "batch 1233 loss : 0.029960952699184418\n",
      "batch 1234 loss : 0.23405992984771729\n",
      "batch 1235 loss : 0.10603749752044678\n",
      "batch 1236 loss : 0.03468211367726326\n",
      "batch 1237 loss : 0.12403415888547897\n",
      "batch 1238 loss : 0.19401367008686066\n",
      "batch 1239 loss : 0.13930797576904297\n",
      "batch 1240 loss : 0.0115060955286026\n",
      "batch 1241 loss : 0.12812146544456482\n",
      "batch 1242 loss : 0.01303861103951931\n",
      "batch 1243 loss : 0.07435157895088196\n",
      "batch 1244 loss : 0.06977515667676926\n",
      "batch 1245 loss : 0.060890983790159225\n",
      "batch 1246 loss : 0.3103339374065399\n",
      "batch 1247 loss : 0.19916081428527832\n",
      "batch 1248 loss : 0.07992131263017654\n",
      "batch 1249 loss : 0.09552676975727081\n",
      "batch 1250 loss : 0.07949820160865784\n",
      "batch 1251 loss : 0.07470721006393433\n",
      "batch 1252 loss : 0.13989251852035522\n",
      "batch 1253 loss : 0.03284551203250885\n",
      "batch 1254 loss : 0.053822048008441925\n",
      "batch 1255 loss : 0.04930562153458595\n",
      "batch 1256 loss : 0.10830871760845184\n",
      "batch 1257 loss : 0.05614347383379936\n",
      "batch 1258 loss : 0.025131339207291603\n",
      "batch 1259 loss : 0.07520261406898499\n",
      "batch 1260 loss : 0.16966117918491364\n",
      "batch 1261 loss : 0.018593447282910347\n",
      "batch 1262 loss : 0.12996642291545868\n",
      "batch 1263 loss : 0.03340238705277443\n",
      "batch 1264 loss : 0.024878669530153275\n",
      "batch 1265 loss : 0.3436127305030823\n",
      "batch 1266 loss : 0.04168681055307388\n",
      "batch 1267 loss : 0.018500294536352158\n",
      "batch 1268 loss : 0.013512123376131058\n",
      "batch 1269 loss : 0.013966505415737629\n",
      "batch 1270 loss : 0.022008346393704414\n",
      "batch 1271 loss : 0.09052324295043945\n",
      "batch 1272 loss : 0.009307016618549824\n",
      "batch 1273 loss : 0.0381389819085598\n",
      "batch 1274 loss : 0.007396601606160402\n",
      "batch 1275 loss : 0.021855851635336876\n",
      "batch 1276 loss : 0.021068008616566658\n",
      "batch 1277 loss : 0.13140769302845\n",
      "batch 1278 loss : 0.01880001276731491\n",
      "batch 1279 loss : 0.014252298511564732\n",
      "batch 1280 loss : 0.05383661761879921\n",
      "batch 1281 loss : 0.05499327555298805\n",
      "batch 1282 loss : 0.061576925218105316\n",
      "batch 1283 loss : 0.02577197551727295\n",
      "batch 1284 loss : 0.08754191547632217\n",
      "batch 1285 loss : 0.20093068480491638\n",
      "batch 1286 loss : 0.08893581479787827\n",
      "batch 1287 loss : 0.02774464152753353\n",
      "batch 1288 loss : 0.08462696522474289\n",
      "batch 1289 loss : 0.008458849973976612\n",
      "batch 1290 loss : 0.005021810065954924\n",
      "batch 1291 loss : 0.18120303750038147\n",
      "batch 1292 loss : 0.09314820915460587\n",
      "batch 1293 loss : 0.1795034110546112\n",
      "batch 1294 loss : 0.02968388795852661\n",
      "batch 1295 loss : 0.06471624225378036\n",
      "batch 1296 loss : 0.007612507324665785\n",
      "batch 1297 loss : 0.19777296483516693\n",
      "batch 1298 loss : 0.08238134533166885\n",
      "batch 1299 loss : 0.0710386261343956\n",
      "batch 1300 loss : 0.06946456432342529\n",
      "batch 1301 loss : 0.04272644594311714\n",
      "batch 1302 loss : 0.11582139134407043\n",
      "batch 1303 loss : 0.004468889907002449\n",
      "batch 1304 loss : 0.018548160791397095\n",
      "batch 1305 loss : 0.029671097174286842\n",
      "batch 1306 loss : 0.02330959215760231\n",
      "batch 1307 loss : 0.02906576730310917\n",
      "batch 1308 loss : 0.05662110820412636\n",
      "batch 1309 loss : 0.03805301710963249\n",
      "batch 1310 loss : 0.026564890518784523\n",
      "batch 1311 loss : 0.17693620920181274\n",
      "batch 1312 loss : 0.2193596065044403\n",
      "batch 1313 loss : 0.2150285840034485\n",
      "batch 1314 loss : 0.12695930898189545\n",
      "batch 1315 loss : 0.15087828040122986\n",
      "batch 1316 loss : 0.08095235377550125\n",
      "batch 1317 loss : 0.4109084904193878\n",
      "batch 1318 loss : 0.2845227122306824\n",
      "batch 1319 loss : 0.13398443162441254\n",
      "batch 1320 loss : 0.14424578845500946\n",
      "batch 1321 loss : 0.25508788228034973\n",
      "batch 1322 loss : 0.028106745332479477\n",
      "batch 1323 loss : 0.044001542031764984\n",
      "batch 1324 loss : 0.030489720404148102\n",
      "batch 1325 loss : 0.03227832168340683\n",
      "batch 1326 loss : 0.17577895522117615\n",
      "batch 1327 loss : 0.07352498918771744\n",
      "batch 1328 loss : 0.016292117536067963\n",
      "batch 1329 loss : 0.013337770476937294\n",
      "batch 1330 loss : 0.006971918512135744\n",
      "batch 1331 loss : 0.028162579983472824\n",
      "batch 1332 loss : 0.009183763526380062\n",
      "batch 1333 loss : 0.0368262343108654\n",
      "batch 1334 loss : 0.01728864200413227\n",
      "batch 1335 loss : 0.008430209942162037\n",
      "batch 1336 loss : 0.030182167887687683\n",
      "batch 1337 loss : 0.01529806386679411\n",
      "batch 1338 loss : 0.013802582398056984\n",
      "batch 1339 loss : 0.007652684580534697\n",
      "batch 1340 loss : 0.09144173562526703\n",
      "batch 1341 loss : 0.04217080399394035\n",
      "batch 1342 loss : 0.09232605248689651\n",
      "batch 1343 loss : 0.06836007535457611\n",
      "batch 1344 loss : 0.24137213826179504\n",
      "batch 1345 loss : 0.0979883000254631\n",
      "batch 1346 loss : 0.05688086524605751\n",
      "batch 1347 loss : 0.036641933023929596\n",
      "batch 1348 loss : 0.06254451721906662\n",
      "batch 1349 loss : 0.012462460435926914\n",
      "batch 1350 loss : 0.11626191437244415\n",
      "batch 1351 loss : 0.17165157198905945\n",
      "batch 1352 loss : 0.1083708107471466\n",
      "batch 1353 loss : 0.081351637840271\n",
      "batch 1354 loss : 0.0167497880756855\n",
      "batch 1355 loss : 0.12799851596355438\n",
      "batch 1356 loss : 0.04760786145925522\n",
      "batch 1357 loss : 0.07880819588899612\n",
      "batch 1358 loss : 0.09913824498653412\n",
      "batch 1359 loss : 0.17881761491298676\n",
      "batch 1360 loss : 0.07683540135622025\n",
      "batch 1361 loss : 0.1456581950187683\n",
      "batch 1362 loss : 0.09937243908643723\n",
      "batch 1363 loss : 0.08065294474363327\n",
      "batch 1364 loss : 0.06677137315273285\n",
      "batch 1365 loss : 0.10649903118610382\n",
      "batch 1366 loss : 0.25493016839027405\n",
      "batch 1367 loss : 0.03124675527215004\n",
      "batch 1368 loss : 0.05263231322169304\n",
      "batch 1369 loss : 0.11161285638809204\n",
      "batch 1370 loss : 0.05299757793545723\n",
      "batch 1371 loss : 0.11557657271623611\n",
      "batch 1372 loss : 0.25477123260498047\n",
      "batch 1373 loss : 0.07796820998191833\n",
      "batch 1374 loss : 0.024574346840381622\n",
      "batch 1375 loss : 0.04233841970562935\n",
      "batch 1376 loss : 0.17741690576076508\n",
      "batch 1377 loss : 0.02370952069759369\n",
      "batch 1378 loss : 0.03337078541517258\n",
      "batch 1379 loss : 0.0949084535241127\n",
      "batch 1380 loss : 0.09169536083936691\n",
      "batch 1381 loss : 0.11020948737859726\n",
      "batch 1382 loss : 0.304654061794281\n",
      "batch 1383 loss : 0.32790669798851013\n",
      "batch 1384 loss : 0.04097630828619003\n",
      "batch 1385 loss : 0.4076089560985565\n",
      "batch 1386 loss : 0.3672722280025482\n",
      "batch 1387 loss : 0.017715055495500565\n",
      "batch 1388 loss : 0.026045333594083786\n",
      "batch 1389 loss : 0.27161842584609985\n",
      "batch 1390 loss : 0.013005225919187069\n",
      "batch 1391 loss : 0.017317600548267365\n",
      "batch 1392 loss : 0.17689423263072968\n",
      "batch 1393 loss : 0.10277702659368515\n",
      "batch 1394 loss : 0.059456877410411835\n",
      "batch 1395 loss : 0.057453058660030365\n",
      "batch 1396 loss : 0.08710719645023346\n",
      "batch 1397 loss : 0.07565470039844513\n",
      "batch 1398 loss : 0.6374525427818298\n",
      "batch 1399 loss : 0.1524345874786377\n",
      "batch 1400 loss : 0.10215038061141968\n",
      "batch 1401 loss : 0.07160790264606476\n",
      "batch 1402 loss : 0.041893597692251205\n",
      "batch 1403 loss : 0.033357810229063034\n",
      "batch 1404 loss : 0.3577740490436554\n",
      "batch 1405 loss : 0.4063311815261841\n",
      "batch 1406 loss : 0.05278085917234421\n",
      "batch 1407 loss : 0.02631986141204834\n",
      "batch 1408 loss : 0.09903459995985031\n",
      "batch 1409 loss : 0.12565802037715912\n",
      "batch 1410 loss : 0.05559360235929489\n",
      "batch 1411 loss : 0.014961672015488148\n",
      "batch 1412 loss : 0.1463037133216858\n",
      "batch 1413 loss : 0.04251464083790779\n",
      "batch 1414 loss : 0.21774092316627502\n",
      "batch 1415 loss : 0.12755168974399567\n",
      "batch 1416 loss : 0.21951062977313995\n",
      "batch 1417 loss : 0.02117270603775978\n",
      "batch 1418 loss : 0.03904539719223976\n",
      "batch 1419 loss : 0.02409631572663784\n",
      "batch 1420 loss : 0.09970788657665253\n",
      "batch 1421 loss : 0.08799967169761658\n",
      "batch 1422 loss : 0.027527421712875366\n",
      "batch 1423 loss : 0.4440104067325592\n",
      "batch 1424 loss : 0.1628609001636505\n",
      "batch 1425 loss : 0.12402085959911346\n",
      "batch 1426 loss : 0.050153736025094986\n",
      "batch 1427 loss : 0.013945439830422401\n",
      "batch 1428 loss : 0.23066914081573486\n",
      "batch 1429 loss : 0.012003306299448013\n",
      "batch 1430 loss : 0.05286836624145508\n",
      "batch 1431 loss : 0.08237091451883316\n",
      "batch 1432 loss : 0.11006726324558258\n",
      "batch 1433 loss : 0.10273300856351852\n",
      "batch 1434 loss : 0.007187648676335812\n",
      "batch 1435 loss : 0.09470302611589432\n",
      "batch 1436 loss : 0.10860071331262589\n",
      "batch 1437 loss : 0.05550872161984444\n",
      "batch 1438 loss : 0.033899419009685516\n",
      "batch 1439 loss : 0.13554134964942932\n",
      "batch 1440 loss : 0.02081768959760666\n",
      "batch 1441 loss : 0.02448105439543724\n",
      "batch 1442 loss : 0.15256421267986298\n",
      "batch 1443 loss : 0.13075019419193268\n",
      "batch 1444 loss : 0.07702229917049408\n",
      "batch 1445 loss : 0.09499137103557587\n",
      "batch 1446 loss : 0.11984618753194809\n",
      "batch 1447 loss : 0.06566193699836731\n",
      "batch 1448 loss : 0.021253153681755066\n",
      "batch 1449 loss : 0.10920564830303192\n",
      "batch 1450 loss : 0.02121128886938095\n",
      "batch 1451 loss : 0.0338386595249176\n",
      "batch 1452 loss : 0.13207219541072845\n",
      "batch 1453 loss : 0.1119464784860611\n",
      "batch 1454 loss : 0.04258059710264206\n",
      "batch 1455 loss : 0.010202635079622269\n",
      "batch 1456 loss : 0.1433556079864502\n",
      "batch 1457 loss : 0.05627695843577385\n",
      "batch 1458 loss : 0.031362809240818024\n",
      "batch 1459 loss : 0.024203667417168617\n",
      "batch 1460 loss : 0.08350445330142975\n",
      "batch 1461 loss : 0.07031705975532532\n",
      "batch 1462 loss : 0.3302418291568756\n",
      "batch 1463 loss : 0.025545042008161545\n",
      "batch 1464 loss : 0.03774406388401985\n",
      "batch 1465 loss : 0.030795415863394737\n",
      "batch 1466 loss : 0.052202314138412476\n",
      "batch 1467 loss : 0.015155992470681667\n",
      "batch 1468 loss : 0.028484297916293144\n",
      "batch 1469 loss : 0.014672121964395046\n",
      "batch 1470 loss : 0.07569780200719833\n",
      "batch 1471 loss : 0.0649823546409607\n",
      "batch 1472 loss : 0.12858739495277405\n",
      "batch 1473 loss : 0.048987314105033875\n",
      "batch 1474 loss : 0.11837182193994522\n",
      "batch 1475 loss : 0.08474626392126083\n",
      "batch 1476 loss : 0.05026337504386902\n",
      "batch 1477 loss : 0.020027728751301765\n",
      "batch 1478 loss : 0.18664701282978058\n",
      "batch 1479 loss : 0.011749613098800182\n",
      "batch 1480 loss : 0.049318794161081314\n",
      "batch 1481 loss : 0.19150519371032715\n",
      "batch 1482 loss : 0.014820265583693981\n",
      "batch 1483 loss : 0.01635989360511303\n",
      "batch 1484 loss : 0.02030986361205578\n",
      "batch 1485 loss : 0.009763071313500404\n",
      "batch 1486 loss : 0.09888944774866104\n",
      "batch 1487 loss : 0.06877783685922623\n",
      "batch 1488 loss : 0.043571989983320236\n",
      "batch 1489 loss : 0.01835322007536888\n",
      "batch 1490 loss : 0.12261941283941269\n",
      "batch 1491 loss : 0.09511972963809967\n",
      "batch 1492 loss : 0.03249475359916687\n",
      "batch 1493 loss : 0.05051584169268608\n",
      "batch 1494 loss : 0.034566983580589294\n",
      "batch 1495 loss : 0.018981941044330597\n",
      "batch 1496 loss : 0.09737129509449005\n",
      "batch 1497 loss : 0.0850486233830452\n",
      "batch 1498 loss : 0.3112204074859619\n",
      "batch 1499 loss : 0.02715841867029667\n",
      "batch 1500 loss : 0.023359324783086777\n",
      "batch 1501 loss : 0.14872516691684723\n",
      "batch 1502 loss : 0.024513578042387962\n",
      "batch 1503 loss : 0.006779300980269909\n",
      "batch 1504 loss : 0.293681263923645\n",
      "batch 1505 loss : 0.10453251749277115\n",
      "batch 1506 loss : 0.02267497032880783\n",
      "batch 1507 loss : 0.23225782811641693\n",
      "batch 1508 loss : 0.31635305285453796\n",
      "batch 1509 loss : 0.07337454706430435\n",
      "batch 1510 loss : 0.14397558569908142\n",
      "batch 1511 loss : 0.14468739926815033\n",
      "batch 1512 loss : 0.12360981106758118\n",
      "batch 1513 loss : 0.028950626030564308\n",
      "batch 1514 loss : 0.11929066479206085\n",
      "batch 1515 loss : 0.10729935020208359\n",
      "batch 1516 loss : 0.04234129562973976\n",
      "batch 1517 loss : 0.019809316843748093\n",
      "batch 1518 loss : 0.08108588308095932\n",
      "batch 1519 loss : 0.1174934059381485\n",
      "batch 1520 loss : 0.15930810570716858\n",
      "batch 1521 loss : 0.0631236806511879\n",
      "batch 1522 loss : 0.17868080735206604\n",
      "batch 1523 loss : 0.11906679719686508\n",
      "batch 1524 loss : 0.0410541333258152\n",
      "batch 1525 loss : 0.03334404155611992\n",
      "batch 1526 loss : 0.0689925104379654\n",
      "batch 1527 loss : 0.02368241362273693\n",
      "batch 1528 loss : 0.24125221371650696\n",
      "batch 1529 loss : 0.017838070169091225\n",
      "batch 1530 loss : 0.06559531390666962\n",
      "batch 1531 loss : 0.4394617974758148\n",
      "batch 1532 loss : 0.054256197065114975\n",
      "batch 1533 loss : 0.19157978892326355\n",
      "batch 1534 loss : 0.07328982651233673\n",
      "batch 1535 loss : 0.01386241614818573\n",
      "batch 1536 loss : 0.03426216542720795\n",
      "batch 1537 loss : 0.2216968834400177\n",
      "batch 1538 loss : 0.0702674612402916\n",
      "batch 1539 loss : 0.2072151154279709\n",
      "batch 1540 loss : 0.02443685196340084\n",
      "batch 1541 loss : 0.08053780347108841\n",
      "batch 1542 loss : 0.014778335578739643\n",
      "batch 1543 loss : 0.013760958798229694\n",
      "batch 1544 loss : 0.01623268611729145\n",
      "batch 1545 loss : 0.04385044425725937\n",
      "batch 1546 loss : 0.008765146136283875\n",
      "batch 1547 loss : 0.04255254939198494\n",
      "batch 1548 loss : 0.009080316871404648\n",
      "batch 1549 loss : 0.04674449935555458\n",
      "batch 1550 loss : 0.09764627367258072\n",
      "batch 1551 loss : 0.0982726514339447\n",
      "batch 1552 loss : 0.053500592708587646\n",
      "batch 1553 loss : 0.17219974100589752\n",
      "batch 1554 loss : 0.09489218145608902\n",
      "batch 1555 loss : 0.15472112596035004\n",
      "batch 1556 loss : 0.030210886150598526\n",
      "batch 1557 loss : 0.04189541935920715\n",
      "batch 1558 loss : 0.07072383165359497\n",
      "batch 1559 loss : 0.0472039133310318\n",
      "batch 1560 loss : 0.1163361445069313\n",
      "batch 1561 loss : 0.07574337720870972\n",
      "batch 1562 loss : 0.1899476945400238\n",
      "batch 1563 loss : 0.04921139404177666\n",
      "batch 1564 loss : 0.004228715319186449\n",
      "batch 1565 loss : 0.1079537644982338\n",
      "batch 1566 loss : 0.007780076470226049\n",
      "batch 1567 loss : 0.5133243203163147\n",
      "batch 1568 loss : 0.05536313354969025\n",
      "batch 1569 loss : 0.06399022787809372\n",
      "batch 1570 loss : 0.1260046660900116\n",
      "batch 1571 loss : 0.05472925305366516\n",
      "batch 1572 loss : 0.038586392998695374\n",
      "batch 1573 loss : 0.026052754372358322\n",
      "batch 1574 loss : 0.03677722439169884\n",
      "batch 1575 loss : 0.02488471008837223\n",
      "batch 1576 loss : 0.0935136154294014\n",
      "batch 1577 loss : 0.04632684588432312\n",
      "batch 1578 loss : 0.01662369817495346\n",
      "batch 1579 loss : 0.13897189497947693\n",
      "batch 1580 loss : 0.06870991736650467\n",
      "batch 1581 loss : 0.106390580534935\n",
      "batch 1582 loss : 0.23874931037425995\n",
      "batch 1583 loss : 0.12576156854629517\n",
      "batch 1584 loss : 0.05084073916077614\n",
      "batch 1585 loss : 0.018314378336071968\n",
      "batch 1586 loss : 0.019382324069738388\n",
      "batch 1587 loss : 0.12739397585391998\n",
      "batch 1588 loss : 0.13175897300243378\n",
      "batch 1589 loss : 0.03158437833189964\n",
      "batch 1590 loss : 0.035104233771562576\n",
      "batch 1591 loss : 0.03619696572422981\n",
      "batch 1592 loss : 0.03391178324818611\n",
      "batch 1593 loss : 0.01872234232723713\n",
      "batch 1594 loss : 0.014924615621566772\n",
      "batch 1595 loss : 0.4690379798412323\n",
      "batch 1596 loss : 0.024764016270637512\n",
      "batch 1597 loss : 0.03205250948667526\n",
      "batch 1598 loss : 0.06377111375331879\n",
      "batch 1599 loss : 0.2613564133644104\n",
      "batch 1600 loss : 0.008431920781731606\n",
      "batch 1601 loss : 0.059784721583127975\n",
      "batch 1602 loss : 0.1817147433757782\n",
      "batch 1603 loss : 0.2390180230140686\n",
      "batch 1604 loss : 0.07432954013347626\n",
      "batch 1605 loss : 0.11490646749734879\n",
      "batch 1606 loss : 0.036206088960170746\n",
      "batch 1607 loss : 0.11013805866241455\n",
      "batch 1608 loss : 0.09625693410634995\n",
      "batch 1609 loss : 0.08666065335273743\n",
      "batch 1610 loss : 0.020356029272079468\n",
      "batch 1611 loss : 0.02474597841501236\n",
      "batch 1612 loss : 0.03032449632883072\n",
      "batch 1613 loss : 0.08448226004838943\n",
      "batch 1614 loss : 0.025921113789081573\n",
      "batch 1615 loss : 0.019946636632084846\n",
      "batch 1616 loss : 0.020428793504834175\n",
      "batch 1617 loss : 0.005157602950930595\n",
      "batch 1618 loss : 0.04835629090666771\n",
      "batch 1619 loss : 0.1188199520111084\n",
      "batch 1620 loss : 0.04035326838493347\n",
      "batch 1621 loss : 0.22172674536705017\n",
      "batch 1622 loss : 0.12717370688915253\n",
      "batch 1623 loss : 0.032233644276857376\n",
      "batch 1624 loss : 0.025478113442659378\n",
      "batch 1625 loss : 0.008430984802544117\n",
      "batch 1626 loss : 0.036166850477457047\n",
      "batch 1627 loss : 0.029334478080272675\n",
      "batch 1628 loss : 0.2736722230911255\n",
      "batch 1629 loss : 0.17042802274227142\n",
      "batch 1630 loss : 0.01014627330005169\n",
      "batch 1631 loss : 0.051422495394945145\n",
      "batch 1632 loss : 0.06504622101783752\n",
      "batch 1633 loss : 0.03245898708701134\n",
      "batch 1634 loss : 0.009549492970108986\n",
      "batch 1635 loss : 0.14421911537647247\n",
      "batch 1636 loss : 0.0730692595243454\n",
      "batch 1637 loss : 0.06534718722105026\n",
      "batch 1638 loss : 0.03477003425359726\n",
      "batch 1639 loss : 0.032187022268772125\n",
      "batch 1640 loss : 0.18218731880187988\n",
      "batch 1641 loss : 0.08288019150495529\n",
      "batch 1642 loss : 0.1555085927248001\n",
      "batch 1643 loss : 0.235209122300148\n",
      "batch 1644 loss : 0.017590563744306564\n",
      "batch 1645 loss : 0.01522019598633051\n",
      "batch 1646 loss : 0.08398193120956421\n",
      "batch 1647 loss : 0.09241627901792526\n",
      "batch 1648 loss : 0.12726588547229767\n",
      "batch 1649 loss : 0.11993083357810974\n",
      "batch 1650 loss : 0.042448803782463074\n",
      "batch 1651 loss : 0.06070418655872345\n",
      "batch 1652 loss : 0.013032549992203712\n",
      "batch 1653 loss : 0.10920827090740204\n",
      "batch 1654 loss : 0.1380428522825241\n",
      "batch 1655 loss : 0.15190380811691284\n",
      "batch 1656 loss : 0.03168828785419464\n",
      "batch 1657 loss : 0.06708895415067673\n",
      "batch 1658 loss : 0.054943542927503586\n",
      "batch 1659 loss : 0.15005327761173248\n",
      "batch 1660 loss : 0.015625199303030968\n",
      "batch 1661 loss : 0.021571233868598938\n",
      "batch 1662 loss : 0.11678564548492432\n",
      "batch 1663 loss : 0.13667172193527222\n",
      "batch 1664 loss : 0.024384476244449615\n",
      "batch 1665 loss : 0.08859678357839584\n",
      "batch 1666 loss : 0.042092882096767426\n",
      "batch 1667 loss : 0.08575707674026489\n",
      "batch 1668 loss : 0.025505894795060158\n",
      "batch 1669 loss : 0.03732820227742195\n",
      "batch 1670 loss : 0.24722503125667572\n",
      "batch 1671 loss : 0.0425860658288002\n",
      "batch 1672 loss : 0.0915728285908699\n",
      "batch 1673 loss : 0.1595613956451416\n",
      "batch 1674 loss : 0.0944477766752243\n",
      "batch 1675 loss : 0.05175567418336868\n",
      "batch 1676 loss : 0.02391784079372883\n",
      "batch 1677 loss : 0.051071569323539734\n",
      "batch 1678 loss : 0.11599402874708176\n",
      "batch 1679 loss : 0.014576764777302742\n",
      "batch 1680 loss : 0.02890564128756523\n",
      "batch 1681 loss : 0.013705510646104813\n",
      "batch 1682 loss : 0.04277214780449867\n",
      "batch 1683 loss : 0.037830650806427\n",
      "batch 1684 loss : 0.1254202276468277\n",
      "batch 1685 loss : 0.17384067177772522\n",
      "batch 1686 loss : 0.21345824003219604\n",
      "batch 1687 loss : 0.19416719675064087\n",
      "batch 1688 loss : 0.08930888772010803\n",
      "batch 1689 loss : 0.08397609740495682\n",
      "batch 1690 loss : 0.008420407772064209\n",
      "batch 1691 loss : 0.009855975396931171\n",
      "batch 1692 loss : 0.3936554491519928\n",
      "batch 1693 loss : 0.32758358120918274\n",
      "batch 1694 loss : 0.12054654955863953\n",
      "batch 1695 loss : 0.07230304926633835\n",
      "batch 1696 loss : 0.02686294913291931\n",
      "batch 1697 loss : 0.12424492090940475\n",
      "batch 1698 loss : 0.05401104316115379\n",
      "batch 1699 loss : 0.04484671726822853\n",
      "batch 1700 loss : 0.0037407679483294487\n",
      "batch 1701 loss : 0.053383972495794296\n",
      "batch 1702 loss : 0.08392681926488876\n",
      "batch 1703 loss : 0.05102809891104698\n",
      "batch 1704 loss : 0.09925024956464767\n",
      "batch 1705 loss : 0.11377187818288803\n",
      "batch 1706 loss : 0.04238029569387436\n",
      "batch 1707 loss : 0.051040127873420715\n",
      "batch 1708 loss : 0.06043485179543495\n",
      "batch 1709 loss : 0.19895640015602112\n",
      "batch 1710 loss : 0.07610633969306946\n",
      "batch 1711 loss : 0.042734287679195404\n",
      "batch 1712 loss : 0.012549448758363724\n",
      "batch 1713 loss : 0.18648138642311096\n",
      "batch 1714 loss : 0.14331702888011932\n",
      "batch 1715 loss : 0.012276804074645042\n",
      "batch 1716 loss : 0.20629529654979706\n",
      "batch 1717 loss : 0.061820246279239655\n",
      "batch 1718 loss : 0.008727888576686382\n",
      "batch 1719 loss : 0.08066852390766144\n",
      "batch 1720 loss : 0.08941074460744858\n",
      "batch 1721 loss : 0.01834583468735218\n",
      "batch 1722 loss : 0.04132913425564766\n",
      "batch 1723 loss : 0.2018345445394516\n",
      "batch 1724 loss : 0.20930242538452148\n",
      "batch 1725 loss : 0.05497366935014725\n",
      "batch 1726 loss : 0.07431071251630783\n",
      "batch 1727 loss : 0.08554238826036453\n",
      "batch 1728 loss : 0.07614319771528244\n",
      "batch 1729 loss : 0.041700221598148346\n",
      "batch 1730 loss : 0.016955580562353134\n",
      "batch 1731 loss : 0.013706156052649021\n",
      "batch 1732 loss : 0.009234562516212463\n",
      "batch 1733 loss : 0.06503210216760635\n",
      "batch 1734 loss : 0.029972929507493973\n",
      "batch 1735 loss : 0.0836258977651596\n",
      "batch 1736 loss : 0.04291621223092079\n",
      "batch 1737 loss : 0.010984551161527634\n",
      "batch 1738 loss : 0.029382895678281784\n",
      "batch 1739 loss : 0.056248776614665985\n",
      "batch 1740 loss : 0.014797021634876728\n",
      "batch 1741 loss : 0.15464842319488525\n",
      "batch 1742 loss : 0.011504675261676311\n",
      "batch 1743 loss : 0.050300925970077515\n",
      "batch 1744 loss : 0.07014591246843338\n",
      "batch 1745 loss : 0.027122531086206436\n",
      "batch 1746 loss : 0.011968398466706276\n",
      "batch 1747 loss : 0.027138352394104004\n",
      "batch 1748 loss : 0.041787032037973404\n",
      "batch 1749 loss : 0.03750808909535408\n",
      "batch 1750 loss : 0.05380983278155327\n",
      "batch 1751 loss : 0.016919447109103203\n",
      "batch 1752 loss : 0.06589187681674957\n",
      "batch 1753 loss : 0.4276127517223358\n",
      "batch 1754 loss : 0.053694140166044235\n",
      "batch 1755 loss : 0.031204696744680405\n",
      "batch 1756 loss : 0.1131729930639267\n",
      "batch 1757 loss : 0.27442502975463867\n",
      "batch 1758 loss : 0.08230002224445343\n",
      "batch 1759 loss : 0.16244381666183472\n",
      "batch 1760 loss : 0.19147716462612152\n",
      "batch 1761 loss : 0.03623786196112633\n",
      "batch 1762 loss : 0.04092676565051079\n",
      "batch 1763 loss : 0.15541037917137146\n",
      "batch 1764 loss : 0.018314111977815628\n",
      "batch 1765 loss : 0.048035603016614914\n",
      "batch 1766 loss : 0.2607993483543396\n",
      "batch 1767 loss : 0.017628803849220276\n",
      "batch 1768 loss : 0.10764789581298828\n",
      "batch 1769 loss : 0.023509088903665543\n",
      "batch 1770 loss : 0.30961155891418457\n",
      "batch 1771 loss : 0.058980464935302734\n",
      "batch 1772 loss : 0.02170936018228531\n",
      "batch 1773 loss : 0.009577167220413685\n",
      "batch 1774 loss : 0.10240247845649719\n",
      "batch 1775 loss : 0.024188818410038948\n",
      "batch 1776 loss : 0.032219938933849335\n",
      "batch 1777 loss : 0.04201813042163849\n",
      "batch 1778 loss : 0.041587620973587036\n",
      "batch 1779 loss : 0.019143257290124893\n",
      "batch 1780 loss : 0.1677173227071762\n",
      "batch 1781 loss : 0.09229425340890884\n",
      "batch 1782 loss : 0.06032010540366173\n",
      "batch 1783 loss : 0.12050662189722061\n",
      "batch 1784 loss : 0.12875968217849731\n",
      "batch 1785 loss : 0.171708881855011\n",
      "batch 1786 loss : 0.1069784015417099\n",
      "batch 1787 loss : 0.04279760271310806\n",
      "batch 1788 loss : 0.13151325285434723\n",
      "batch 1789 loss : 0.09998021274805069\n",
      "batch 1790 loss : 0.4985630214214325\n",
      "batch 1791 loss : 0.02419121377170086\n",
      "batch 1792 loss : 0.1857500970363617\n",
      "batch 1793 loss : 0.13651606440544128\n",
      "batch 1794 loss : 0.2353934794664383\n",
      "batch 1795 loss : 0.013954298570752144\n",
      "batch 1796 loss : 0.05173158273100853\n",
      "batch 1797 loss : 0.011546219699084759\n",
      "batch 1798 loss : 0.05011654272675514\n",
      "batch 1799 loss : 0.09225306659936905\n",
      "batch 1800 loss : 0.12639418244361877\n",
      "batch 1801 loss : 0.0730431079864502\n",
      "batch 1802 loss : 0.00727525120601058\n",
      "batch 1803 loss : 0.01878936216235161\n",
      "batch 1804 loss : 0.03862020745873451\n",
      "batch 1805 loss : 0.06191890686750412\n",
      "batch 1806 loss : 0.20140407979488373\n",
      "batch 1807 loss : 0.09337726980447769\n",
      "batch 1808 loss : 0.03325195610523224\n",
      "batch 1809 loss : 0.04278925061225891\n",
      "batch 1810 loss : 0.04985051602125168\n",
      "batch 1811 loss : 0.04203062132000923\n",
      "batch 1812 loss : 0.12866762280464172\n",
      "batch 1813 loss : 0.0641263946890831\n",
      "batch 1814 loss : 0.012794604524970055\n",
      "batch 1815 loss : 0.21040000021457672\n",
      "batch 1816 loss : 0.08734798431396484\n",
      "batch 1817 loss : 0.06218437850475311\n",
      "batch 1818 loss : 0.020298611372709274\n",
      "batch 1819 loss : 0.005463127046823502\n",
      "batch 1820 loss : 0.03641952946782112\n",
      "batch 1821 loss : 0.045460231602191925\n",
      "batch 1822 loss : 0.08587446063756943\n",
      "batch 1823 loss : 0.14266982674598694\n",
      "batch 1824 loss : 0.054305076599121094\n",
      "batch 1825 loss : 0.03940640389919281\n",
      "batch 1826 loss : 0.09159795939922333\n",
      "batch 1827 loss : 0.1582004576921463\n",
      "batch 1828 loss : 0.07823513448238373\n",
      "batch 1829 loss : 0.09553509950637817\n",
      "batch 1830 loss : 0.15915881097316742\n",
      "batch 1831 loss : 0.12104222178459167\n",
      "batch 1832 loss : 0.17102709412574768\n",
      "batch 1833 loss : 0.06840097904205322\n",
      "batch 1834 loss : 0.14087553322315216\n",
      "batch 1835 loss : 0.08467717468738556\n",
      "batch 1836 loss : 0.035950012505054474\n",
      "batch 1837 loss : 0.25805413722991943\n",
      "batch 1838 loss : 0.07482945919036865\n",
      "batch 1839 loss : 0.07838467508554459\n",
      "batch 1840 loss : 0.07459615916013718\n",
      "batch 1841 loss : 0.13390345871448517\n",
      "batch 1842 loss : 0.07266633957624435\n",
      "batch 1843 loss : 0.024809299036860466\n",
      "batch 1844 loss : 0.008093563839793205\n",
      "batch 1845 loss : 0.09863849729299545\n",
      "batch 1846 loss : 0.1267608255147934\n",
      "batch 1847 loss : 0.07795274257659912\n",
      "batch 1848 loss : 0.15036508440971375\n",
      "batch 1849 loss : 0.05242431163787842\n",
      "batch 1850 loss : 0.045978888869285583\n",
      "batch 1851 loss : 0.06410157680511475\n",
      "batch 1852 loss : 0.07281893491744995\n",
      "batch 1853 loss : 0.03983437269926071\n",
      "batch 1854 loss : 0.02850479632616043\n",
      "batch 1855 loss : 0.09710485488176346\n",
      "batch 1856 loss : 0.1091838926076889\n",
      "batch 1857 loss : 0.13965240120887756\n",
      "batch 1858 loss : 0.12175177782773972\n",
      "batch 1859 loss : 0.19116251170635223\n",
      "batch 1860 loss : 0.008578336797654629\n",
      "batch 1861 loss : 0.02229202724993229\n",
      "batch 1862 loss : 0.060000259429216385\n",
      "batch 1863 loss : 0.17252150177955627\n",
      "batch 1864 loss : 0.09999680519104004\n",
      "batch 1865 loss : 0.011157493107020855\n",
      "batch 1866 loss : 0.08394327014684677\n",
      "batch 1867 loss : 0.3484433889389038\n",
      "batch 1868 loss : 0.033778171986341476\n",
      "batch 1869 loss : 0.014192872680723667\n",
      "batch 1870 loss : 0.19980914890766144\n",
      "batch 1871 loss : 0.26118364930152893\n",
      "batch 1872 loss : 0.08251843601465225\n",
      "batch 1873 loss : 0.14104768633842468\n",
      "batch 1874 loss : 0.10350078344345093\n",
      "Epoch 0 test loss : 0.05801066351537324\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "                        \n",
    "        loss = F.nll_loss(output, target)\n",
    "                        # 예측, #정답\n",
    "                        # y.shape 사이즈가 ohe 되지 않았기 때문에 사용\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        print('batch {} loss : {}'.format(batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target).item()\n",
    "    test_loss /= (len(test_loader.dataset) // batch_size)\n",
    "\n",
    "    print('Epoch {} test loss : {}'.format(epoch, test_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e42283cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해설용\n",
    "# for epoch in range(10):\n",
    "#     # Train Mode\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "#         data, target = data.to(device), target.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         output = model(data)\n",
    "#         loss = F.cross_entropy(output, target)\n",
    "        \n",
    "#         loss.backward()\n",
    "        \n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if batch_idx % 1000 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100 * batch_idx / len(train_loader), loss.item(), end=\"\\r\"\n",
    "#             ))\n",
    "            \n",
    "#     model.eval()\n",
    "\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "#             pred = output.argmax(dim=1, keepdim=True)\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print('\\nTest set: Average Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818de4b",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03bd5b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2fba6468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0579, Accuracy: 9824/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test mode\n",
    "model.eval() \n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c0d33",
   "metadata": {},
   "source": [
    "- autograd engine, 즉 backpropagatin이나 gradient 계산 등을 꺼서 memory usage를 줄이고 속도를 높임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109fdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992477ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43060b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf405ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb3633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5a26d0e877c4652cf53eb6b13536f4959f02e722fb5eef7979a29d14fb02c8a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('TF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
